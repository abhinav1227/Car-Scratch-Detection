{"cells":[{"cell_type":"code","execution_count":null,"id":"N5S9t0UJA4_o","metadata":{"executionInfo":{"elapsed":3035,"status":"ok","timestamp":1672942473224,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"N5S9t0UJA4_o"},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2"]},{"cell_type":"markdown","id":"c14b3237","metadata":{"id":"c14b3237"},"source":["# installing and import dependencies"]},{"cell_type":"code","execution_count":2,"id":"TAn0VZtKnQSZ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22991,"status":"ok","timestamp":1672942498577,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"TAn0VZtKnQSZ","outputId":"43f8579b-9f21-493e-b4d3-51ad6867bc4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"YB5GyftNnhuW","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":558,"status":"ok","timestamp":1672942503907,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"YB5GyftNnhuW","outputId":"6cb9ccff-d36d-4e98-d48b-791375e148b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Zummit/Zummit\n"]}],"source":["cd /content/drive/MyDrive/Zummit/Zummit"]},{"cell_type":"code","execution_count":4,"id":"gUgWh6_GnsCa","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1672942504368,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"gUgWh6_GnsCa","outputId":"fc9eff1e-c70b-44c1-f18c-8f2b6a17ddcd"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Zummit/Zummit'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"markdown","id":"7b41746e","metadata":{"id":"7b41746e"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"id":"6cbc4290","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1793,"status":"ok","timestamp":1672649966821,"user":{"displayName":"aniket malviya","userId":"06013323772686351302"},"user_tz":-330},"id":"6cbc4290","outputId":"40893159-2797-4719-94d1-fafe0c0a5c28"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n","YOLOv5 🚀 2023-1-2 Python-3.8.16 torch-1.13.0+cu116 CPU\n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n","Adding AutoShape... \n"]}],"source":["model = torch.hub.load('ultralytics/yolov5','yolov5s')"]},{"cell_type":"code","execution_count":null,"id":"0d398a31","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d398a31","outputId":"2b9bb276-7e07-4665-9817-bf5f27cfb5b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00047), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 193.18it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 611.58it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.01 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.512-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1265    0.04365          0         80        320: 100% 28/28 [00:07<00:00,  3.68it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G      0.108    0.05273          0         75        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09649    0.05356          0         71        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09086    0.05368          0         85        320: 100% 28/28 [00:07<00:00,  3.98it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G     0.0827    0.05306          0         88        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G     0.0779    0.05062          0        107        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07409    0.05149          0         83        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07135    0.05227          0         82        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06879    0.05001          0         71        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06678    0.04952          0         75        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.19it/s]\n","                   all        145        235      0.606      0.465      0.475      0.173\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m97 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.60595,              0.46459,              0.47489,              0.17284,             0.068033,             0.020861,                    0,              0.00636,              0.01057,              0.96517,              0.00047,               2.6839,              0.78895,              0.08766,              0.05872,              0.45533,               1.2081,               1.3015,              0.99959,                  0.2,               3.9592,                    0,              0.01413,              0.73726,              0.47284,                    0,              0.09223,              0.49545,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00558, lrf=0.01067, momentum=0.9568, weight_decay=0.00048, warmup_epochs=2.53511, warmup_momentum=0.82416, warmup_bias_lr=0.09065, box=0.05507, cls=0.45174, cls_pw=1.13103, obj=1.41915, obj_pw=0.98657, iou_t=0.2, anchor_t=4.05609, fl_gamma=0.0, hsv_h=0.0135, hsv_s=0.73084, hsv_v=0.47801, degrees=0.0, translate=0.08421, scale=0.56368, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00558) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 192.08it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 583.12it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.07 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.509-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1197    0.04687          0         80        320: 100% 28/28 [00:07<00:00,  3.55it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G      0.103    0.05647          0         77        320: 100% 28/28 [00:09<00:00,  3.10it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09224    0.05745          0         70        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08673    0.05826          0         88        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.07994    0.05706          0         92        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07442    0.05468          0        111        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07074    0.05551          0         81        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06872    0.05611          0         88        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06575    0.05419          0         72        320: 100% 28/28 [00:08<00:00,  3.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G     0.0643    0.05443          0         73        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.48it/s]\n","                   all        145        235       0.51      0.426       0.39       0.13\n","\n","10 epochs completed in 0.021 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m98 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.51034,              0.42553,              0.39001,                 0.13,             0.066171,             0.024116,                    0,              0.00558,              0.01067,               0.9568,              0.00048,               2.5351,              0.82416,              0.09065,              0.05507,              0.45174,                1.131,               1.4191,              0.98657,                  0.2,               4.0561,                    0,               0.0135,              0.73084,              0.47801,                    0,              0.08421,              0.56368,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0076, lrf=0.01289, momentum=0.96301, weight_decay=0.00048, warmup_epochs=2.7154, warmup_momentum=0.92932, warmup_bias_lr=0.07194, box=0.05188, cls=0.48399, cls_pw=1.00122, obj=1.2945, obj_pw=0.89576, iou_t=0.2, anchor_t=3.4105, fl_gamma=0.0, hsv_h=0.01189, hsv_s=0.77767, hsv_v=0.51933, degrees=0.0, translate=0.09203, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.92082, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0076) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 190.38it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7500: 100% 1000/1000 [00:01<00:00, 528.67it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.29: 1.0000 best possible recall, 4.58 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.459/0.750-mean/best, past_thr=0.535-mean: 30,25, 57,36, 59,69, 120,52, 93,100, 157,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1081    0.03703          0         76        320: 100% 28/28 [00:07<00:00,  3.88it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G    0.08792    0.04548          0         80        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.07961    0.04434          0         67        320: 100% 28/28 [00:06<00:00,  4.29it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.07396      0.043          0         75        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.06887     0.0431          0         86        320: 100% 28/28 [00:06<00:00,  4.32it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.06549    0.04202          0         93        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.06291    0.04197          0         66        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06046    0.04114          0         88        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.05869    0.04085          0        104        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.05741    0.04065          0         87        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.31it/s]\n","                   all        145        235      0.616      0.451      0.472      0.171\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m99 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.61649,              0.45147,              0.47176,              0.17099,             0.057075,              0.01834,                    0,               0.0076,              0.01289,              0.96301,              0.00048,               2.7154,              0.92932,              0.07194,              0.05188,              0.48399,               1.0012,               1.2945,              0.89576,                  0.2,               3.4105,                    0,              0.01189,              0.77767,              0.51933,                    0,              0.09203,              0.49617,                    0,                    0,                    0,                  0.5,              0.92082,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00615, lrf=0.01024, momentum=0.91568, weight_decay=0.00048, warmup_epochs=2.56999, warmup_momentum=0.7972, warmup_bias_lr=0.09576, box=0.06218, cls=0.36856, cls_pw=1.32138, obj=1.51969, obj_pw=0.99963, iou_t=0.2, anchor_t=3.80254, fl_gamma=0.0, hsv_h=0.01379, hsv_s=0.8019, hsv_v=0.46895, degrees=0.0, translate=0.09059, scale=0.49498, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.28134\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.28134\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00615) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 188.90it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 562.37it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 1.0000 best possible recall, 4.93 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.516-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1331    0.05051          0         80        320: 100% 28/28 [00:07<00:00,  3.75it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1134    0.06081          0         75        320: 100% 28/28 [00:07<00:00,  3.92it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1024     0.0614          0         71        320: 100% 28/28 [00:08<00:00,  3.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G     0.0966    0.06181          0         85        320: 100% 28/28 [00:07<00:00,  3.98it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08909    0.06044          0         88        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08377    0.05786          0        106        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07968    0.05902          0         83        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07679    0.05957          0         83        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07354    0.05753          0         71        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07155    0.05742          0         75        320: 100% 28/28 [00:08<00:00,  3.21it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.13it/s]\n","                   all        145        235      0.571       0.46      0.442      0.155\n","\n","10 epochs completed in 0.021 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m100 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m               0.571,              0.45957,              0.44248,              0.15514,             0.073352,             0.024186,                    0,              0.00615,              0.01024,              0.91568,              0.00048,                 2.57,               0.7972,              0.09576,              0.06218,              0.36856,               1.3214,               1.5197,              0.99963,                  0.2,               3.8025,                    0,              0.01379,               0.8019,              0.46895,                    0,              0.09059,              0.49498,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.2813\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00659, lrf=0.01049, momentum=0.96757, weight_decay=0.00047, warmup_epochs=2.7154, warmup_momentum=0.78895, warmup_bias_lr=0.08647, box=0.05927, cls=0.4619, cls_pw=1.20972, obj=1.34271, obj_pw=1.00406, iou_t=0.2, anchor_t=4.15407, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.7391, hsv_v=0.47901, degrees=0.0, translate=0.0961, scale=0.51651, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.98213, mixup=0.0, copy_paste=0.0, anchors=2.0953\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0953\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00659) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00047), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 182.46it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:02<00:00, 467.15it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 1.0000 best possible recall, 5.13 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.506-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1287    0.04468          0         73        320: 100% 28/28 [00:07<00:00,  3.80it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1096    0.05494          0         79        320: 100% 28/28 [00:06<00:00,  4.00it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09891    0.05619          0         75        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09328    0.05444          0         90        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08537    0.05376          0         93        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08001    0.05384          0        103        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07547    0.05458          0         88        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07306    0.05325          0         73        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06996    0.05232          0         85        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06944    0.05254          0         73        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.08it/s]\n","                   all        145        235      0.499      0.468       0.43      0.151\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m101 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m              0.4989,              0.46809,              0.43011,              0.15052,             0.071969,             0.023401,                    0,              0.00659,              0.01049,              0.96757,              0.00047,               2.7154,              0.78895,              0.08647,              0.05927,               0.4619,               1.2097,               1.3427,               1.0041,                  0.2,               4.1541,                    0,              0.01413,               0.7391,              0.47901,                    0,               0.0961,              0.51651,                    0,                    0,                    0,                  0.5,              0.98213,                    0,                    0,               2.0953\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00663, lrf=0.01063, momentum=0.96446, weight_decay=0.00047, warmup_epochs=2.76087, warmup_momentum=0.77976, warmup_bias_lr=0.08388, box=0.05767, cls=0.47941, cls_pw=1.21841, obj=1.25321, obj_pw=0.98657, iou_t=0.2, anchor_t=4.05192, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.7391, hsv_v=0.48181, degrees=0.0, translate=0.09231, scale=0.49864, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.01273\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.01273\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00663) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00047), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 188.77it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 587.77it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.07 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.509-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1248    0.04169          0         80        320: 100% 28/28 [00:07<00:00,  3.74it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1065    0.05067          0         75        320: 100% 28/28 [00:06<00:00,  4.00it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09557    0.05157          0         71        320: 100% 28/28 [00:06<00:00,  4.02it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G     0.0898    0.05197          0         85        320: 100% 28/28 [00:08<00:00,  3.36it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08204    0.05084          0         88        320: 100% 28/28 [00:07<00:00,  3.82it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07711    0.04866          0        106        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07318     0.0499          0         83        320: 100% 28/28 [00:06<00:00,  4.10it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07087    0.05046          0         83        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06803    0.04843          0         71        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06612      0.048          0         75        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.07it/s]\n","                   all        145        235      0.605      0.519      0.481      0.157\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m102 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.60491,              0.51915,              0.48118,               0.1565,             0.067463,             0.020157,                    0,              0.00663,              0.01063,              0.96446,              0.00047,               2.7609,              0.77976,              0.08388,              0.05767,              0.47941,               1.2184,               1.2532,              0.98657,                  0.2,               4.0519,                    0,              0.01413,               0.7391,              0.48181,                    0,              0.09231,              0.49864,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.0127\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0059, lrf=0.01, momentum=0.94452, weight_decay=0.00044, warmup_epochs=2.60621, warmup_momentum=0.75416, warmup_bias_lr=0.08425, box=0.0596, cls=0.44882, cls_pw=1.30392, obj=1.2945, obj_pw=1.02, iou_t=0.2, anchor_t=4.30534, fl_gamma=0.0, hsv_h=0.01361, hsv_s=0.71302, hsv_v=0.44783, degrees=0.0, translate=0.09231, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.9408, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0059) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00044), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 188.26it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 509.12it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.23: 1.0000 best possible recall, 5.21 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.502-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1309    0.04293          0         76        320: 100% 28/28 [00:07<00:00,  3.78it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1128    0.05287          0         83        320: 100% 28/28 [00:07<00:00,  3.90it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1008    0.05423          0         85        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09462     0.0538          0         67        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08826    0.05202          0         80        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G     0.0827    0.05296          0        102        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07772    0.05245          0         72        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07484    0.05163          0         97        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07284    0.05088          0         68        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G     0.0701    0.04996          0         83        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.23it/s]\n","                   all        145        235      0.609      0.417      0.428      0.166\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m103 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.60948,              0.41702,              0.42756,                0.166,              0.07044,             0.022266,                    0,               0.0059,                 0.01,              0.94452,              0.00044,               2.6062,              0.75416,              0.08425,               0.0596,              0.44882,               1.3039,               1.2945,                 1.02,                  0.2,               4.3053,                    0,              0.01361,              0.71302,              0.44783,                    0,              0.09231,              0.49617,                    0,                    0,                    0,                  0.5,               0.9408,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00705, lrf=0.01493, momentum=0.97192, weight_decay=0.00045, warmup_epochs=2.27398, warmup_momentum=0.72615, warmup_bias_lr=0.09818, box=0.0516, cls=0.45201, cls_pw=1.2201, obj=1.71351, obj_pw=0.89564, iou_t=0.2, anchor_t=3.47405, fl_gamma=0.0, hsv_h=0.01653, hsv_s=0.84206, hsv_v=0.56906, degrees=0.0, translate=0.09086, scale=0.4984, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.83642, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00705) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00045), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 189.80it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7499: 100% 1000/1000 [00:02<00:00, 488.47it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.29: 1.0000 best possible recall, 4.63 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.458/0.750-mean/best, past_thr=0.531-mean: 30,25, 58,37, 56,71, 119,52, 93,99, 157,122\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1104    0.04547          0         70        320: 100% 28/28 [00:07<00:00,  3.73it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G    0.09482     0.0544          0         81        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.08517    0.05536          0         60        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.07937     0.0561          0         64        320: 100% 28/28 [00:06<00:00,  4.34it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.07354    0.05446          0         91        320: 100% 28/28 [00:06<00:00,  4.02it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.06711    0.05383          0         89        320: 100% 28/28 [00:08<00:00,  3.28it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.06501    0.05451          0         91        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06268    0.05176          0         64        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06086    0.05264          0         67        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.05883    0.05262          0         74        320: 100% 28/28 [00:06<00:00,  4.27it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.93it/s]\n","                   all        145        235      0.528      0.471      0.432       0.15\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m104 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.52797,              0.47121,              0.43164,              0.14984,             0.059744,              0.02459,                    0,              0.00705,              0.01493,              0.97192,              0.00045,                2.274,              0.72615,              0.09818,               0.0516,              0.45201,               1.2201,               1.7135,              0.89564,                  0.2,               3.4741,                    0,              0.01653,              0.84206,              0.56906,                    0,              0.09086,               0.4984,                    0,                    0,                    0,                  0.5,              0.83642,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00735, lrf=0.01, momentum=0.96301, weight_decay=0.00037, warmup_epochs=3.16181, warmup_momentum=0.71589, warmup_bias_lr=0.08084, box=0.06289, cls=0.45146, cls_pw=1.17281, obj=1.31419, obj_pw=1.05989, iou_t=0.2, anchor_t=4.28156, fl_gamma=0.0, hsv_h=0.01553, hsv_s=0.64375, hsv_v=0.40436, degrees=0.0, translate=0.07969, scale=0.38909, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.78865, mixup=0.0, copy_paste=0.0, anchors=2.32736\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.32736\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00735) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00037), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 185.62it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:02<00:00, 397.67it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.23: 1.0000 best possible recall, 5.20 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.503-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1355    0.04262          0         63        320: 100% 28/28 [00:07<00:00,  3.76it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1152    0.05151          0         72        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1052    0.05347          0         89        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09803    0.05281          0         70        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.09051     0.0514          0         67        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08458    0.05012          0         70        320: 100% 28/28 [00:06<00:00,  4.29it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.08018    0.05093          0         58        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07681    0.05144          0         83        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07388     0.0504          0         65        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07182    0.04712          0         71        320: 100% 28/28 [00:06<00:00,  4.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.38it/s]\n","                   all        145        235      0.445      0.523      0.416      0.135\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m105 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.44513,               0.5234,              0.41609,              0.13526,             0.077705,             0.023378,                    0,              0.00735,                 0.01,              0.96301,              0.00037,               3.1618,              0.71589,              0.08084,              0.06289,              0.45146,               1.1728,               1.3142,               1.0599,                  0.2,               4.2816,                    0,              0.01553,              0.64375,              0.40436,                    0,              0.07969,              0.38909,                    0,                    0,                    0,                  0.5,              0.78865,                    0,                    0,               2.3274\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00622, lrf=0.01066, momentum=0.93379, weight_decay=0.00048, warmup_epochs=3.3713, warmup_momentum=0.95, warmup_bias_lr=0.08766, box=0.05829, cls=0.43058, cls_pw=1.24189, obj=1.34507, obj_pw=0.98657, iou_t=0.2, anchor_t=4.34005, fl_gamma=0.0, hsv_h=0.01636, hsv_s=0.63851, hsv_v=0.4884, degrees=0.0, translate=0.086, scale=0.52893, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.95074, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00622) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 191.66it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 511.86it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.23: 1.0000 best possible recall, 5.23 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.501-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1255    0.04463          0         76        320: 100% 28/28 [00:07<00:00,  3.75it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1047    0.05568          0         82        320: 100% 28/28 [00:07<00:00,  3.95it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09293    0.05614          0         97        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08792    0.05419          0         80        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08215     0.0525          0         98        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07846    0.05383          0        114        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07409    0.05297          0         79        320: 100% 28/28 [00:08<00:00,  3.32it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07144    0.05269          0         91        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06995    0.05161          0         86        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06818    0.05121          0         85        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.14it/s]\n","                   all        145        235      0.567      0.469      0.457      0.159\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m106 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.56742,              0.46887,              0.45693,              0.15898,             0.068381,              0.02278,                    0,              0.00622,              0.01066,              0.93379,              0.00048,               3.3713,                 0.95,              0.08766,              0.05829,              0.43058,               1.2419,               1.3451,              0.98657,                  0.2,                 4.34,                    0,              0.01636,              0.63851,               0.4884,                    0,                0.086,              0.52893,                    0,                    0,                    0,                  0.5,              0.95074,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00509, lrf=0.01055, momentum=0.97274, weight_decay=0.00038, warmup_epochs=2.61668, warmup_momentum=0.83881, warmup_bias_lr=0.09496, box=0.04819, cls=0.4747, cls_pw=1.2201, obj=1.15643, obj_pw=1.12614, iou_t=0.2, anchor_t=4.84268, fl_gamma=0.0, hsv_h=0.01671, hsv_s=0.85602, hsv_v=0.64048, degrees=0.0, translate=0.09203, scale=0.36445, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.90089, mixup=0.0, copy_paste=0.0, anchors=2.42884\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.42884\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00509) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00038), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 189.67it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 526.37it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.21: 1.0000 best possible recall, 5.45 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.489-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1061    0.04079          0         75        320: 100% 28/28 [00:09<00:00,  2.95it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G    0.09103    0.05162          0         66        320: 100% 28/28 [00:07<00:00,  3.98it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.0826    0.05264          0         76        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.07687    0.05137          0         68        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G     0.0708    0.05157          0         74        320: 100% 28/28 [00:06<00:00,  4.33it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.06669    0.05148          0        108        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.06345    0.05049          0         92        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06167    0.04909          0         79        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.05983    0.04899          0         73        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.05796    0.05031          0         93        320: 100% 28/28 [00:06<00:00,  4.32it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.21it/s]\n","                   all        145        235      0.529      0.474       0.45      0.141\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m107 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.52929,              0.47371,              0.45022,                0.141,             0.059688,             0.022851,                    0,              0.00509,              0.01055,              0.97274,              0.00038,               2.6167,              0.83881,              0.09496,              0.04819,               0.4747,               1.2201,               1.1564,               1.1261,                  0.2,               4.8427,                    0,              0.01671,              0.85602,              0.64048,                    0,              0.09203,              0.36445,                    0,                    0,                    0,                  0.5,              0.90089,                    0,                    0,               2.4288\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00471, lrf=0.01, momentum=0.93737, weight_decay=0.00041, warmup_epochs=2.2347, warmup_momentum=0.66739, warmup_bias_lr=0.08997, box=0.05829, cls=0.49568, cls_pw=0.78948, obj=1.27042, obj_pw=0.98657, iou_t=0.2, anchor_t=3.44443, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.67486, hsv_v=0.41545, degrees=0.0, translate=0.0788, scale=0.50454, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00471) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00041), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 192.58it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7499: 100% 1000/1000 [00:01<00:00, 608.54it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.29: 1.0000 best possible recall, 4.60 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.459/0.750-mean/best, past_thr=0.533-mean: 30,25, 57,36, 59,68, 121,52, 93,101, 159,120\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1264    0.03861          0         80        320: 100% 28/28 [00:07<00:00,  3.71it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G      0.112    0.04605          0         75        320: 100% 28/28 [00:07<00:00,  3.79it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1001    0.04889          0         71        320: 100% 28/28 [00:06<00:00,  4.10it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09255    0.05088          0         85        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08608    0.04904          0         88        320: 100% 28/28 [00:06<00:00,  4.01it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08016    0.04671          0        107        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07597    0.04759          0         83        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07273    0.04766          0         84        320: 100% 28/28 [00:07<00:00,  3.65it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06993     0.0457          0         71        320: 100% 28/28 [00:07<00:00,  3.50it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G     0.0677    0.04577          0         75        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.23it/s]\n","                   all        145        235      0.452      0.445      0.394      0.133\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m108 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.45166,              0.44515,               0.3944,               0.1334,             0.068249,             0.019459,                    0,              0.00471,                 0.01,              0.93737,              0.00041,               2.2347,              0.66739,              0.08997,              0.05829,              0.49568,              0.78948,               1.2704,              0.98657,                  0.2,               3.4444,                    0,              0.01413,              0.67486,              0.41545,                    0,               0.0788,              0.50454,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01, momentum=0.98, weight_decay=0.00049, warmup_epochs=2.82434, warmup_momentum=0.62709, warmup_bias_lr=0.09805, box=0.04559, cls=0.35927, cls_pw=1.02543, obj=1.20503, obj_pw=1.06932, iou_t=0.2, anchor_t=4.20589, fl_gamma=0.0, hsv_h=0.01518, hsv_s=0.7391, hsv_v=0.47057, degrees=0.0, translate=0.07967, scale=0.48458, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.85727, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00049), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 188.02it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 571.43it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 1.0000 best possible recall, 5.15 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.505-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1007    0.03841          0         71        320: 100% 28/28 [00:07<00:00,  3.86it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.0897    0.04616          0         91        320: 100% 28/28 [00:07<00:00,  3.82it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.08039    0.05013          0         86        320: 100% 28/28 [00:08<00:00,  3.49it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.07344    0.04904          0         44        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.06852    0.04883          0         78        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.06319    0.04879          0        103        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.05984    0.04867          0         84        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G     0.0587    0.04657          0         94        320: 100% 28/28 [00:06<00:00,  4.27it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.05614    0.04538          0         71        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.05458    0.04616          0         92        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.36it/s]\n","                   all        145        235      0.557      0.489      0.447      0.147\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m109 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.55716,              0.48936,              0.44742,              0.14699,             0.054971,             0.022595,                    0,              0.00636,                 0.01,                 0.98,              0.00049,               2.8243,              0.62709,              0.09805,              0.04559,              0.35927,               1.0254,                1.205,               1.0693,                  0.2,               4.2059,                    0,              0.01518,               0.7391,              0.47057,                    0,              0.07967,              0.48458,                    0,                    0,                    0,                  0.5,              0.85727,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01042, momentum=0.96301, weight_decay=0.00048, warmup_epochs=2.66044, warmup_momentum=0.79528, warmup_bias_lr=0.08943, box=0.05829, cls=0.46089, cls_pw=1.26835, obj=1.35358, obj_pw=1.02775, iou_t=0.2, anchor_t=4.0012, fl_gamma=0.0, hsv_h=0.0146, hsv_s=0.70308, hsv_v=0.47057, degrees=0.0, translate=0.09257, scale=0.46931, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.94004, mixup=0.0, copy_paste=0.0, anchors=2.04154\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.04154\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 195.26it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 551.83it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1259    0.04502          0         76        320: 100% 28/28 [00:07<00:00,  3.82it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1067    0.05506          0         84        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09528    0.05455          0         69        320: 100% 28/28 [00:06<00:00,  4.31it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08949    0.05363          0         72        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08311    0.05291          0         84        320: 100% 28/28 [00:06<00:00,  4.27it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07727    0.05343          0        100        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07339    0.05314          0         72        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07072    0.05216          0         92        320: 100% 28/28 [00:06<00:00,  4.29it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06857    0.05237          0         71        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06587    0.05068          0         84        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.53it/s]\n","                   all        145        235      0.519      0.494       0.45      0.163\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m110 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.51869,              0.49362,              0.45022,              0.16272,             0.067787,             0.022695,                    0,              0.00636,              0.01042,              0.96301,              0.00048,               2.6604,              0.79528,              0.08943,              0.05829,              0.46089,               1.2684,               1.3536,               1.0277,                  0.2,               4.0012,                    0,               0.0146,              0.70308,              0.47057,                    0,              0.09257,              0.46931,                    0,                    0,                    0,                  0.5,              0.94004,                    0,                    0,               2.0415\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00604, lrf=0.01055, momentum=0.94889, weight_decay=0.00054, warmup_epochs=2.40587, warmup_momentum=0.76329, warmup_bias_lr=0.08766, box=0.05202, cls=0.50908, cls_pw=1.33167, obj=1.2945, obj_pw=1.0382, iou_t=0.2, anchor_t=3.6224, fl_gamma=0.0, hsv_h=0.01294, hsv_s=0.66354, hsv_v=0.54098, degrees=0.0, translate=0.09267, scale=0.45189, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.46005\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.46005\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00604) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00054), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 196.84it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7499: 100% 1000/1000 [00:01<00:00, 617.18it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.28: 1.0000 best possible recall, 4.78 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.525-mean: 30,26, 57,36, 59,69, 120,52, 94,100, 157,120\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1116    0.04286          0         79        320: 100% 28/28 [00:07<00:00,  3.72it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G    0.09634    0.05131          0         74        320: 100% 28/28 [00:07<00:00,  4.00it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.08592    0.05302          0         69        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08076    0.05283          0         81        320: 100% 28/28 [00:07<00:00,  3.60it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.07429     0.0518          0         87        320: 100% 28/28 [00:07<00:00,  3.73it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.06935    0.04944          0        103        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G     0.0656    0.05069          0         84        320: 100% 28/28 [00:06<00:00,  4.33it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06292    0.05085          0         83        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06038    0.04894          0         73        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.05847    0.04855          0         75        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.45it/s]\n","                   all        145        235      0.503      0.498       0.43       0.15\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m111 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.50294,              0.49787,              0.42977,              0.14952,             0.059708,             0.020946,                    0,              0.00604,              0.01055,              0.94889,              0.00054,               2.4059,              0.76329,              0.08766,              0.05202,              0.50908,               1.3317,               1.2945,               1.0382,                  0.2,               3.6224,                    0,              0.01294,              0.66354,              0.54098,                    0,              0.09267,              0.45189,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                 2.46\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00613, lrf=0.01, momentum=0.93562, weight_decay=0.00049, warmup_epochs=2.69792, warmup_momentum=0.8101, warmup_bias_lr=0.0861, box=0.05829, cls=0.44503, cls_pw=1.3399, obj=1.52187, obj_pw=0.96858, iou_t=0.2, anchor_t=4.0012, fl_gamma=0.0, hsv_h=0.01481, hsv_s=0.69102, hsv_v=0.48569, degrees=0.0, translate=0.08893, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00613) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00049), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 196.56it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 544.26it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1256    0.04986          0         80        320: 100% 28/28 [00:07<00:00,  3.70it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1071    0.06007          0         75        320: 100% 28/28 [00:06<00:00,  4.01it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09633    0.06083          0         71        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09062    0.06136          0         85        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08334    0.06025          0         88        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G     0.0782     0.0575          0        106        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07423    0.05846          0         83        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07141    0.05931          0         83        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G     0.0688    0.05733          0         70        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06714    0.05696          0         75        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.21it/s]\n","                   all        145        235      0.494       0.47      0.413      0.153\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m112 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.49423,              0.46988,              0.41324,              0.15311,             0.067431,             0.024478,                    0,              0.00613,                 0.01,              0.93562,              0.00049,               2.6979,               0.8101,               0.0861,              0.05829,              0.44503,               1.3399,               1.5219,              0.96858,                  0.2,               4.0012,                    0,              0.01481,              0.69102,              0.48569,                    0,              0.08893,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00594, lrf=0.01, momentum=0.96732, weight_decay=0.00048, warmup_epochs=2.63539, warmup_momentum=0.78895, warmup_bias_lr=0.08971, box=0.06226, cls=0.49843, cls_pw=1.18294, obj=1.23392, obj_pw=0.98657, iou_t=0.2, anchor_t=4.0012, fl_gamma=0.0, hsv_h=0.01437, hsv_s=0.78262, hsv_v=0.42933, degrees=0.0, translate=0.08931, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.98651, mixup=0.0, copy_paste=0.0, anchors=2.00224\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.00224\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00594) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 191.44it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 581.56it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1343    0.04069          0         73        320: 100% 28/28 [00:07<00:00,  3.72it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1152    0.04931          0         73        320: 100% 28/28 [00:07<00:00,  3.89it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1029    0.05137          0         78        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09722    0.04917          0         87        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08927    0.05021          0         97        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08405    0.04879          0        102        320: 100% 28/28 [00:08<00:00,  3.27it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07992    0.04918          0         87        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07651    0.04798          0         71        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07277    0.04746          0         78        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07138    0.04778          0         70        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.25it/s]\n","                   all        145        235      0.513      0.479      0.434      0.144\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m113 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.51293,              0.47949,              0.43355,              0.14388,             0.071598,             0.021174,                    0,              0.00594,                 0.01,              0.96732,              0.00048,               2.6354,              0.78895,              0.08971,              0.06226,              0.49843,               1.1829,               1.2339,              0.98657,                  0.2,               4.0012,                    0,              0.01437,              0.78262,              0.42933,                    0,              0.08931,              0.49617,                    0,                    0,                    0,                  0.5,              0.98651,                    0,                    0,               2.0022\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00591, lrf=0.01055, momentum=0.98, weight_decay=0.00048, warmup_epochs=2.7154, warmup_momentum=0.83967, warmup_bias_lr=0.08766, box=0.05766, cls=0.46089, cls_pw=1.16559, obj=1.2945, obj_pw=1.09986, iou_t=0.2, anchor_t=3.37432, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.63505, hsv_v=0.49575, degrees=0.0, translate=0.08664, scale=0.51783, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.8994, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00591) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 194.39it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7497: 100% 1000/1000 [00:01<00:00, 518.41it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.30: 1.0000 best possible recall, 4.55 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.459/0.750-mean/best, past_thr=0.536-mean: 31,25, 57,36, 58,70, 123,53, 92,100, 158,122\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1215     0.0417          0         76        320: 100% 28/28 [00:07<00:00,  3.79it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1023    0.05235          0         69        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09178    0.05197          0         79        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08659    0.04925          0         69        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08017    0.04935          0         77        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07509    0.04985          0        113        320: 100% 28/28 [00:06<00:00,  4.27it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07145    0.04869          0         91        320: 100% 28/28 [00:06<00:00,  4.32it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06933    0.04714          0         86        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06706    0.04695          0         73        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06563    0.04813          0         93        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.40it/s]\n","                   all        145        235      0.484      0.421      0.402      0.129\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m114 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.48356,              0.42128,              0.40164,              0.12937,             0.070199,             0.022169,                    0,              0.00591,              0.01055,                 0.98,              0.00048,               2.7154,              0.83967,              0.08766,              0.05766,              0.46089,               1.1656,               1.2945,               1.0999,                  0.2,               3.3743,                    0,              0.01413,              0.63505,              0.49575,                    0,              0.08664,              0.51783,                    0,                    0,                    0,                  0.5,               0.8994,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00781, lrf=0.01083, momentum=0.89014, weight_decay=0.00053, warmup_epochs=3.23973, warmup_momentum=0.78895, warmup_bias_lr=0.08766, box=0.07044, cls=0.33598, cls_pw=1.28212, obj=1.42521, obj_pw=0.97439, iou_t=0.2, anchor_t=4.53401, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.65532, hsv_v=0.55809, degrees=0.0, translate=0.08636, scale=0.56612, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.88622, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00781) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00053), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 185.42it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 649.91it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.22: 1.0000 best possible recall, 5.32 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.496-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G      0.153    0.04617          0         74        320: 100% 28/28 [00:07<00:00,  3.80it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1303    0.05693          0         82        320: 100% 28/28 [00:08<00:00,  3.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1188    0.05671          0         87        320: 100% 28/28 [00:06<00:00,  4.29it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G     0.1134     0.0561          0         68        320: 100% 28/28 [00:06<00:00,  4.35it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G     0.1056    0.05484          0         83        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.09914    0.05415          0         67        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.09489    0.05489          0         80        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.09044    0.05353          0         86        320: 100% 28/28 [00:08<00:00,  3.49it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.08735    0.05357          0         66        320: 100% 28/28 [00:07<00:00,  3.99it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.08609    0.05327          0         67        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.16it/s]\n","                   all        145        235      0.556      0.464      0.447      0.151\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m115 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.55609,              0.46383,              0.44691,              0.15138,             0.086455,             0.023971,                    0,              0.00781,              0.01083,              0.89014,              0.00053,               3.2397,              0.78895,              0.08766,              0.07044,              0.33598,               1.2821,               1.4252,              0.97439,                  0.2,                4.534,                    0,              0.01413,              0.65532,              0.55809,                    0,              0.08636,              0.56612,                    0,                    0,                    0,                  0.5,              0.88622,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00592, lrf=0.01025, momentum=0.97872, weight_decay=0.0005, warmup_epochs=2.66546, warmup_momentum=0.83355, warmup_bias_lr=0.0844, box=0.06135, cls=0.45328, cls_pw=1.2201, obj=1.31376, obj_pw=1.02843, iou_t=0.2, anchor_t=4.08288, fl_gamma=0.0, hsv_h=0.01469, hsv_s=0.77574, hsv_v=0.49006, degrees=0.0, translate=0.09101, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.92085, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00592) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 188.97it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 578.00it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 1.0000 best possible recall, 5.08 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.509-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1322    0.04419          0         76        320: 100% 28/28 [00:07<00:00,  3.85it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1111    0.05452          0         80        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G      0.101    0.05449          0         67        320: 100% 28/28 [00:06<00:00,  4.32it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09368    0.05279          0         75        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08709    0.05309          0         86        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08214    0.05215          0         94        320: 100% 28/28 [00:06<00:00,  4.29it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07869    0.05168          0         67        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07633    0.05139          0         88        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07352    0.05077          0        104        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07171     0.0501          0         87        320: 100% 28/28 [00:06<00:00,  4.31it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.32it/s]\n","                   all        145        235      0.563      0.468      0.423      0.138\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m116 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.56312,              0.46809,               0.4227,               0.1381,             0.074045,             0.022447,                    0,              0.00592,              0.01025,              0.97872,               0.0005,               2.6655,              0.83355,               0.0844,              0.06135,              0.45328,               1.2201,               1.3138,               1.0284,                  0.2,               4.0829,                    0,              0.01469,              0.77574,              0.49006,                    0,              0.09101,              0.49617,                    0,                    0,                    0,                  0.5,              0.92085,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00706, lrf=0.01182, momentum=0.98, weight_decay=0.00039, warmup_epochs=3.3994, warmup_momentum=0.66314, warmup_bias_lr=0.08701, box=0.04307, cls=0.27363, cls_pw=1.2201, obj=1.32699, obj_pw=1.17001, iou_t=0.2, anchor_t=4.78753, fl_gamma=0.0, hsv_h=0.01569, hsv_s=0.62383, hsv_v=0.45341, degrees=0.0, translate=0.11103, scale=0.63476, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.98445, mixup=0.0, copy_paste=0.0, anchors=2.58533\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.58533\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n","Model summary: 214 layers, 7022326 parameters, 7022326 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00706) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00039), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 190.79it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.05 anchors/target, 0.015 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7798: 100% 1000/1000 [00:01<00:00, 540.26it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.21: 1.0000 best possible recall, 7.93 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=320, metric_all=0.444/0.780-mean/best, past_thr=0.482-mean: 30,25, 53,36, 57,66, 95,49, 158,42, 79,99, 130,82, 129,145, 219,101\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.85G     0.1008    0.04309          0         77        320: 100% 28/28 [00:07<00:00,  3.65it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.85G    0.08886    0.05347          0         82        320: 100% 28/28 [00:07<00:00,  3.98it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.85G    0.07749     0.0603          0         75        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.85G    0.07117    0.05873          0         97        320: 100% 28/28 [00:07<00:00,  3.64it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.85G    0.06551    0.05837          0         99        320: 100% 28/28 [00:07<00:00,  3.51it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.85G    0.06096    0.05834          0        111        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.85G    0.05751    0.05933          0         92        320: 100% 28/28 [00:06<00:00,  4.00it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.85G    0.05572    0.05888          0         87        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.85G    0.05385    0.05772          0         83        320: 100% 28/28 [00:06<00:00,  4.00it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.85G    0.05318    0.05762          0         78        320: 100% 28/28 [00:08<00:00,  3.17it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.21it/s]\n","                   all        145        235       0.47      0.434      0.353      0.116\n","\n","10 epochs completed in 0.021 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m117 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.46973,              0.43404,              0.35277,               0.1155,             0.053877,             0.027769,                    0,              0.00706,              0.01182,                 0.98,              0.00039,               3.3994,              0.66314,              0.08701,              0.04307,              0.27363,               1.2201,                1.327,                 1.17,                  0.2,               4.7875,                    0,              0.01569,              0.62383,              0.45341,                    0,              0.11103,              0.63476,                    0,                    0,                    0,                  0.5,              0.98445,                    0,                    0,               2.5853\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00446, lrf=0.01, momentum=0.98, weight_decay=0.00042, warmup_epochs=2.7154, warmup_momentum=0.74367, warmup_bias_lr=0.08582, box=0.05829, cls=0.43931, cls_pw=1.20291, obj=1.46345, obj_pw=0.98657, iou_t=0.2, anchor_t=5.05238, fl_gamma=0.0, hsv_h=0.01681, hsv_s=0.85319, hsv_v=0.32377, degrees=0.0, translate=0.09203, scale=0.56126, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00446) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00042), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 190.15it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 511.17it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.20: 1.0000 best possible recall, 5.53 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.485-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1314    0.04875          0         80        320: 100% 28/28 [00:07<00:00,  3.63it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1165    0.05889          0         77        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1042     0.0623          0         72        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09663      0.064          0         87        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08873    0.06296          0         92        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08365    0.06074          0        110        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07912     0.0616          0         81        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07668    0.06224          0         89        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07422    0.06023          0         72        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07278    0.06053          0         74        320: 100% 28/28 [00:06<00:00,  4.10it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.20it/s]\n","                   all        145        235      0.481      0.472      0.385      0.132\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m118 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.48079,              0.47234,              0.38509,              0.13182,             0.073899,             0.025805,                    0,              0.00446,                 0.01,                 0.98,              0.00042,               2.7154,              0.74367,              0.08582,              0.05829,              0.43931,               1.2029,               1.4634,              0.98657,                  0.2,               5.0524,                    0,              0.01681,              0.85319,              0.32377,                    0,              0.09203,              0.56126,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0068, lrf=0.01, momentum=0.96301, weight_decay=0.00046, warmup_epochs=1.87601, warmup_momentum=0.81314, warmup_bias_lr=0.08272, box=0.04848, cls=0.36128, cls_pw=1.15708, obj=1.59705, obj_pw=1.07867, iou_t=0.2, anchor_t=3.79398, fl_gamma=0.0, hsv_h=0.01054, hsv_s=0.73621, hsv_v=0.57524, degrees=0.0, translate=0.12878, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0068) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00046), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 196.72it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 577.14it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 1.0000 best possible recall, 4.93 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.517-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1041    0.05549          0         82        320: 100% 28/28 [00:07<00:00,  3.70it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G    0.08865    0.06572          0         74        320: 100% 28/28 [00:07<00:00,  3.99it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.07917    0.06605          0         69        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.07434    0.06592          0         85        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.06774    0.06517          0         86        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.06396    0.06276          0        104        320: 100% 28/28 [00:08<00:00,  3.42it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.06051    0.06447          0         83        320: 100% 28/28 [00:07<00:00,  3.95it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G     0.0588    0.06507          0         84        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.05677     0.0625          0         73        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.05505    0.06181          0         77        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.17it/s]\n","                   all        145        235      0.487      0.409      0.388      0.143\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m119 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.48732,              0.40851,              0.38806,              0.14338,             0.058593,               0.0264,                    0,               0.0068,                 0.01,              0.96301,              0.00046,                1.876,              0.81314,              0.08272,              0.04848,              0.36128,               1.1571,               1.5971,               1.0787,                  0.2,                3.794,                    0,              0.01054,              0.73621,              0.57524,                    0,              0.12878,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00602, lrf=0.01143, momentum=0.98, weight_decay=0.00048, warmup_epochs=2.60557, warmup_momentum=0.70611, warmup_bias_lr=0.08766, box=0.08044, cls=0.46089, cls_pw=1.49874, obj=1.41745, obj_pw=1.03461, iou_t=0.2, anchor_t=5.36869, fl_gamma=0.0, hsv_h=0.01521, hsv_s=0.7391, hsv_v=0.45752, degrees=0.0, translate=0.09203, scale=0.31519, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=3.81689\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=3.81689\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, 6, 7]], [128, 256, 512]]\n","Model summary: 214 layers, 7027720 parameters, 7027720 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00602) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 190.53it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.35 anchors/target, 0.089 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 12 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8035: 100% 1000/1000 [00:03<00:00, 299.73it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.19: 1.0000 best possible recall, 10.81 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=12, img_size=320, metric_all=0.434/0.804-mean/best, past_thr=0.466-mean: 27,22, 39,32, 68,32, 50,51, 93,51, 66,77, 166,43, 83,109, 128,81, 118,153, 207,95, 184,173\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.86G     0.1836     0.0446          0         74        320: 100% 28/28 [00:07<00:00,  3.81it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.86G     0.1554    0.05877          0         70        320: 100% 28/28 [00:06<00:00,  4.01it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.86G     0.1363    0.06521          0         65        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.86G     0.1259    0.06584          0         74        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.86G     0.1169    0.06334          0         80        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.86G     0.1125    0.06037          0         90        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.86G     0.1071    0.06171          0         77        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.86G      0.104    0.06146          0         76        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.86G    0.09928    0.06029          0         70        320: 100% 28/28 [00:06<00:00,  4.35it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.86G    0.09754    0.05905          0         72        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.03it/s]\n","                   all        145        235      0.446      0.379      0.351      0.105\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m120 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.44589,              0.37872,              0.35051,               0.1053,              0.10291,             0.027023,                    0,              0.00602,              0.01143,                 0.98,              0.00048,               2.6056,              0.70611,              0.08766,              0.08044,              0.46089,               1.4987,               1.4175,               1.0346,                  0.2,               5.3687,                    0,              0.01521,               0.7391,              0.45752,                    0,              0.09203,              0.31519,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               3.8169\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00695, lrf=0.01101, momentum=0.96301, weight_decay=0.00049, warmup_epochs=3.07384, warmup_momentum=0.80184, warmup_bias_lr=0.08173, box=0.05829, cls=0.43005, cls_pw=1.20754, obj=1.26904, obj_pw=0.91153, iou_t=0.2, anchor_t=4.04544, fl_gamma=0.0, hsv_h=0.01387, hsv_s=0.7391, hsv_v=0.5048, degrees=0.0, translate=0.08927, scale=0.51707, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.99834, mixup=0.0, copy_paste=0.0, anchors=2.36486\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.36486\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00695) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00049), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 193.91it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 542.52it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.06 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.510-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1255    0.03986          0         79        320: 100% 28/28 [00:07<00:00,  3.67it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1064    0.04909          0         75        320: 100% 28/28 [00:07<00:00,  3.87it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09538     0.0493          0         71        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09004    0.04905          0         86        320: 100% 28/28 [00:07<00:00,  3.99it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08243     0.0489          0         88        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07698    0.04698          0        107        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07324    0.04648          0         86        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07077    0.04766          0         80        320: 100% 28/28 [00:08<00:00,  3.39it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G     0.0687     0.0464          0         74        320: 100% 28/28 [00:07<00:00,  3.80it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06643    0.04601          0         64        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.22it/s]\n","                   all        145        235      0.562      0.417      0.429      0.141\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m121 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m              0.5624,              0.41702,                0.429,                0.141,             0.069204,              0.01939,                    0,              0.00695,              0.01101,              0.96301,              0.00049,               3.0738,              0.80184,              0.08173,              0.05829,              0.43005,               1.2075,                1.269,              0.91153,                  0.2,               4.0454,                    0,              0.01387,               0.7391,               0.5048,                    0,              0.08927,              0.51707,                    0,                    0,                    0,                  0.5,              0.99834,                    0,                    0,               2.3649\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00601, lrf=0.01055, momentum=0.95864, weight_decay=0.00046, warmup_epochs=2.7154, warmup_momentum=0.75135, warmup_bias_lr=0.08659, box=0.05729, cls=0.46089, cls_pw=1.25699, obj=1.30982, obj_pw=0.96034, iou_t=0.2, anchor_t=3.78214, fl_gamma=0.0, hsv_h=0.014, hsv_s=0.75867, hsv_v=0.43377, degrees=0.0, translate=0.09274, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.9876, mixup=0.0, copy_paste=0.0, anchors=2.19319\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.19319\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00601) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00046), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 186.96it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:02<00:00, 499.74it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 1.0000 best possible recall, 4.92 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.517-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1237    0.04102          0         73        320: 100% 28/28 [00:09<00:00,  2.93it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1075    0.04961          0         73        320: 100% 28/28 [00:07<00:00,  3.83it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09552    0.05195          0         78        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09024    0.04956          0         87        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08239    0.05058          0         97        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07745    0.04897          0        103        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07288    0.05027          0         86        320: 100% 28/28 [00:07<00:00,  3.95it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06961     0.0487          0         71        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G     0.0668    0.04759          0         78        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06591    0.04783          0         70        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.13it/s]\n","                   all        145        235      0.518      0.502      0.445      0.147\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m122 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m              0.5182,              0.50213,              0.44506,              0.14707,               0.0658,             0.021214,                    0,              0.00601,              0.01055,              0.95864,              0.00046,               2.7154,              0.75135,              0.08659,              0.05729,              0.46089,                1.257,               1.3098,              0.96034,                  0.2,               3.7821,                    0,                0.014,              0.75867,              0.43377,                    0,              0.09274,              0.49617,                    0,                    0,                    0,                  0.5,               0.9876,                    0,                    0,               2.1932\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00661, lrf=0.01024, momentum=0.96301, weight_decay=0.0005, warmup_epochs=2.69593, warmup_momentum=0.85394, warmup_bias_lr=0.09044, box=0.06034, cls=0.43602, cls_pw=1.2201, obj=1.3226, obj_pw=0.98657, iou_t=0.2, anchor_t=4.0012, fl_gamma=0.0, hsv_h=0.01367, hsv_s=0.7391, hsv_v=0.47057, degrees=0.0, translate=0.09056, scale=0.49802, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.03323\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.03323\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00661) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 187.80it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 601.54it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1287    0.04491          0         80        320: 100% 28/28 [00:07<00:00,  3.64it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1076    0.05453          0         75        320: 100% 28/28 [00:07<00:00,  3.91it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09706    0.05418          0         71        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09141    0.05428          0         85        320: 100% 28/28 [00:07<00:00,  4.00it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G     0.0835    0.05351          0         88        320: 100% 28/28 [00:07<00:00,  3.95it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07919      0.051          0        106        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07564    0.05198          0         83        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07296    0.05269          0         83        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07041    0.05056          0         70        320: 100% 28/28 [00:07<00:00,  3.82it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G     0.0681    0.05004          0         75        320: 100% 28/28 [00:08<00:00,  3.42it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.33it/s]\n","                   all        145        235      0.524      0.455      0.445      0.161\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m123 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.52422,              0.45532,              0.44524,              0.16077,             0.071151,             0.021675,                    0,              0.00661,              0.01024,              0.96301,               0.0005,               2.6959,              0.85394,              0.09044,              0.06034,              0.43602,               1.2201,               1.3226,              0.98657,                  0.2,               4.0012,                    0,              0.01367,               0.7391,              0.47057,                    0,              0.09056,              0.49802,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.0332\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00733, lrf=0.01055, momentum=0.89968, weight_decay=0.00056, warmup_epochs=2.7154, warmup_momentum=0.79856, warmup_bias_lr=0.06389, box=0.04212, cls=0.4707, cls_pw=1.43633, obj=1.96162, obj_pw=0.82612, iou_t=0.2, anchor_t=4.47458, fl_gamma=0.0, hsv_h=0.01809, hsv_s=0.7391, hsv_v=0.47057, degrees=0.0, translate=0.06937, scale=0.39007, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.40163\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.40163\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00733) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00056), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 187.72it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 540.93it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.22: 1.0000 best possible recall, 5.29 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.498-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.0923    0.05565          0         76        320: 100% 28/28 [00:07<00:00,  3.72it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G    0.07939     0.0668          0         74        320: 100% 28/28 [00:07<00:00,  3.65it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.07125    0.06891          0         69        320: 100% 28/28 [00:07<00:00,  3.54it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.06701    0.06897          0         78        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.06166    0.06829          0         82        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.05762    0.06589          0         95        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.05465    0.06786          0         82        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.05233    0.06895          0         80        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.05063     0.0669          0         70        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.04925    0.06584          0         74        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.31it/s]\n","                   all        145        235       0.56      0.472      0.458      0.163\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m124 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.56013,              0.47234,              0.45781,              0.16335,              0.05096,             0.028044,                    0,              0.00733,              0.01055,              0.89968,              0.00056,               2.7154,              0.79856,              0.06389,              0.04212,               0.4707,               1.4363,               1.9616,              0.82612,                  0.2,               4.4746,                    0,              0.01809,               0.7391,              0.47057,                    0,              0.06937,              0.39007,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.4016\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00641, lrf=0.01069, momentum=0.96259, weight_decay=0.00048, warmup_epochs=2.70167, warmup_momentum=0.78881, warmup_bias_lr=0.08732, box=0.05829, cls=0.4616, cls_pw=1.23214, obj=1.28927, obj_pw=0.98171, iou_t=0.2, anchor_t=3.94724, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.73971, hsv_v=0.47006, degrees=0.0, translate=0.09061, scale=0.49707, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00641) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 188.13it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 679.29it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.01 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.512-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1256    0.04254          0         80        320: 100% 28/28 [00:07<00:00,  3.65it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1072    0.05163          0         75        320: 100% 28/28 [00:07<00:00,  3.84it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09617    0.05232          0         71        320: 100% 28/28 [00:06<00:00,  4.02it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09029    0.05262          0         85        320: 100% 28/28 [00:07<00:00,  3.97it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08264     0.0517          0         88        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07758    0.04942          0        106        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07367    0.05055          0         83        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07087    0.05068          0         83        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06822    0.04882          0         70        320: 100% 28/28 [00:06<00:00,  4.02it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06621    0.04866          0         75        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.26it/s]\n","                   all        145        235      0.532      0.518      0.462      0.163\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m125 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.53234,              0.51829,              0.46208,              0.16315,             0.066116,             0.020773,                    0,              0.00641,              0.01069,              0.96259,              0.00048,               2.7017,              0.78881,              0.08732,              0.05829,               0.4616,               1.2321,               1.2893,              0.98171,                  0.2,               3.9472,                    0,              0.01413,              0.73971,              0.47006,                    0,              0.09061,              0.49707,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01159, momentum=0.97642, weight_decay=0.00054, warmup_epochs=2.48297, warmup_momentum=0.81985, warmup_bias_lr=0.08905, box=0.05949, cls=0.48441, cls_pw=1.2193, obj=1.31581, obj_pw=0.98657, iou_t=0.2, anchor_t=4.0012, fl_gamma=0.0, hsv_h=0.01362, hsv_s=0.7391, hsv_v=0.47057, degrees=0.0, translate=0.09274, scale=0.48782, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.99246, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00054), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:05<00:00, 160.66it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:02<00:00, 490.68it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1267    0.04378          0         75        320: 100% 28/28 [00:07<00:00,  3.75it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1077    0.05382          0         75        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09647    0.05433          0        109        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09037    0.05366          0         87        320: 100% 28/28 [00:08<00:00,  3.32it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08325    0.05301          0         92        320: 100% 28/28 [00:07<00:00,  3.98it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07859    0.05065          0        103        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07482    0.05135          0         82        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07258    0.05113          0         58        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07075    0.05047          0         85        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06823    0.05008          0         67        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.00it/s]\n","                   all        145        235      0.584      0.481      0.488      0.172\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m126 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.58354,              0.48085,              0.48849,              0.17222,             0.068216,              0.02162,                    0,              0.00636,              0.01159,              0.97642,              0.00054,                2.483,              0.81985,              0.08905,              0.05949,              0.48441,               1.2193,               1.3158,              0.98657,                  0.2,               4.0012,                    0,              0.01362,               0.7391,              0.47057,                    0,              0.09274,              0.48782,                    0,                    0,                    0,                  0.5,              0.99246,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0065, lrf=0.01131, momentum=0.96301, weight_decay=0.00044, warmup_epochs=2.7154, warmup_momentum=0.66739, warmup_bias_lr=0.08767, box=0.05829, cls=0.51589, cls_pw=1.34444, obj=1.42699, obj_pw=1.07235, iou_t=0.2, anchor_t=3.27193, fl_gamma=0.0, hsv_h=0.01311, hsv_s=0.73667, hsv_v=0.46045, degrees=0.0, translate=0.09203, scale=0.54154, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.96431, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0065) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00044), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 189.08it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7499: 100% 1000/1000 [00:01<00:00, 505.17it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.31: 1.0000 best possible recall, 4.49 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.462/0.750-mean/best, past_thr=0.542-mean: 30,24, 54,37, 62,71, 115,46, 101,116, 143,86\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1248    0.04605          0         74        320: 100% 28/28 [00:07<00:00,  3.75it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1085    0.05553          0         89        320: 100% 28/28 [00:07<00:00,  3.97it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.0963    0.05655          0         78        320: 100% 28/28 [00:07<00:00,  3.87it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08998    0.05399          0         77        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08359    0.05273          0         81        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07672     0.0548          0         78        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07146    0.05421          0         85        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06987     0.0523          0         80        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06724    0.05203          0         89        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06507    0.05141          0         80        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.96it/s]\n","                   all        145        235      0.563      0.471      0.436      0.149\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m127 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.56297,              0.47142,              0.43557,              0.14925,             0.068575,             0.022228,                    0,               0.0065,              0.01131,              0.96301,              0.00044,               2.7154,              0.66739,              0.08767,              0.05829,              0.51589,               1.3444,                1.427,               1.0723,                  0.2,               3.2719,                    0,              0.01311,              0.73667,              0.46045,                    0,              0.09203,              0.54154,                    0,                    0,                    0,                  0.5,              0.96431,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00564, lrf=0.01129, momentum=0.96301, weight_decay=0.00046, warmup_epochs=2.35975, warmup_momentum=0.76979, warmup_bias_lr=0.09994, box=0.05768, cls=0.46089, cls_pw=1.40642, obj=1.22783, obj_pw=1.02847, iou_t=0.2, anchor_t=3.97303, fl_gamma=0.0, hsv_h=0.01262, hsv_s=0.70992, hsv_v=0.35562, degrees=0.0, translate=0.08741, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0921\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0921\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00564) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00046), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 188.44it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 635.68it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.02 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.512-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G      0.125    0.04181          0         80        320: 100% 28/28 [00:09<00:00,  2.93it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1078    0.05067          0         75        320: 100% 28/28 [00:07<00:00,  3.96it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09636    0.05219          0         71        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09069    0.05238          0         85        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08269    0.05154          0         88        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G     0.0772    0.04918          0        106        320: 100% 28/28 [00:08<00:00,  3.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07355    0.04983          0         83        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07022     0.0505          0         83        320: 100% 28/28 [00:07<00:00,  3.98it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06758    0.04851          0         70        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06543    0.04811          0         75        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.25it/s]\n","                   all        145        235      0.586      0.472       0.47      0.169\n","\n","10 epochs completed in 0.021 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m128 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.58629,              0.47234,              0.47043,              0.16888,             0.066414,             0.019914,                    0,              0.00564,              0.01129,              0.96301,              0.00046,               2.3598,              0.76979,              0.09994,              0.05768,              0.46089,               1.4064,               1.2278,               1.0285,                  0.2,                3.973,                    0,              0.01262,              0.70992,              0.35562,                    0,              0.08741,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.0921\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01079, momentum=0.96301, weight_decay=0.00048, warmup_epochs=2.90916, warmup_momentum=0.78613, warmup_bias_lr=0.0855, box=0.0598, cls=0.4481, cls_pw=1.25271, obj=1.30156, obj_pw=0.94984, iou_t=0.2, anchor_t=3.82447, fl_gamma=0.0, hsv_h=0.01398, hsv_s=0.69626, hsv_v=0.49886, degrees=0.0, translate=0.09698, scale=0.48611, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 190.79it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 618.62it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 1.0000 best possible recall, 4.94 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.516-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1282    0.04149          0         80        320: 100% 28/28 [00:07<00:00,  3.65it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1093    0.05016          0         75        320: 100% 28/28 [00:07<00:00,  3.89it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09824    0.05101          0         71        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09249     0.0513          0         81        320: 100% 28/28 [00:07<00:00,  3.97it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08495    0.05052          0         87        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07904    0.04831          0        106        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07515    0.04906          0         84        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G     0.0721    0.04971          0         81        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06929    0.04778          0         72        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06729    0.04714          0         75        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.17it/s]\n","                   all        145        235      0.579      0.447      0.451      0.162\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m129 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.57854,              0.44681,              0.45106,              0.16163,             0.070641,             0.019804,                    0,              0.00636,              0.01079,              0.96301,              0.00048,               2.9092,              0.78613,               0.0855,               0.0598,               0.4481,               1.2527,               1.3016,              0.94984,                  0.2,               3.8245,                    0,              0.01398,              0.69626,              0.49886,                    0,              0.09698,              0.48611,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00798, lrf=0.01, momentum=0.98, weight_decay=0.00048, warmup_epochs=2.71279, warmup_momentum=0.64773, warmup_bias_lr=0.08257, box=0.0683, cls=0.56965, cls_pw=0.8852, obj=1.52688, obj_pw=0.98657, iou_t=0.2, anchor_t=3.65327, fl_gamma=0.0, hsv_h=0.00848, hsv_s=0.7391, hsv_v=0.47057, degrees=0.0, translate=0.05846, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.61485, mixup=0.0, copy_paste=0.0, anchors=2.90562\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.90562\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n","Model summary: 214 layers, 7022326 parameters, 7022326 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00798) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 186.41it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.01 anchors/target, 0.003 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7806: 100% 1000/1000 [00:02<00:00, 483.89it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.27: 1.0000 best possible recall, 6.97 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=320, metric_all=0.444/0.781-mean/best, past_thr=0.516-mean: 30,23, 45,37, 87,41, 59,66, 160,46, 78,102, 121,79, 201,94, 134,145\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.85G     0.1491    0.03966          0         76        320: 100% 28/28 [00:07<00:00,  3.89it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.85G     0.1263    0.04804          0         70        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.85G     0.1121    0.05137          0         84        320: 100% 28/28 [00:08<00:00,  3.50it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.85G     0.1053    0.04925          0         55        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.85G    0.09719     0.0475          0         70        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.85G    0.09087    0.04705          0         60        320: 100% 28/28 [00:06<00:00,  4.37it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.85G     0.0863    0.04749          0         77        320: 100% 28/28 [00:06<00:00,  4.43it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.85G    0.08439    0.04717          0         84        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.85G    0.08229    0.04689          0         62        320: 100% 28/28 [00:08<00:00,  3.49it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.85G    0.07955     0.0464          0         74        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.28it/s]\n","                   all        145        235      0.375      0.383      0.318     0.0909\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m130 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.37502,              0.38298,               0.3183,             0.090857,             0.089824,              0.02669,                    0,              0.00798,                 0.01,                 0.98,              0.00048,               2.7128,              0.64773,              0.08257,               0.0683,              0.56965,               0.8852,               1.5269,              0.98657,                  0.2,               3.6533,                    0,              0.00848,               0.7391,              0.47057,                    0,              0.05846,              0.49617,                    0,                    0,                    0,                  0.5,              0.61485,                    0,                    0,               2.9056\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00623, lrf=0.01131, momentum=0.95199, weight_decay=0.00049, warmup_epochs=2.75239, warmup_momentum=0.7875, warmup_bias_lr=0.08781, box=0.05365, cls=0.47701, cls_pw=1.18594, obj=1.30761, obj_pw=0.95253, iou_t=0.2, anchor_t=4.0012, fl_gamma=0.0, hsv_h=0.01408, hsv_s=0.75843, hsv_v=0.48478, degrees=0.0, translate=0.09203, scale=0.50466, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00623) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00049), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 190.23it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 545.65it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1165    0.04179          0         80        320: 100% 28/28 [00:07<00:00,  3.75it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G        0.1    0.05066          0         75        320: 100% 28/28 [00:07<00:00,  3.99it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.08962    0.05182          0         71        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08447    0.05208          0         85        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.07744    0.05126          0         88        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07266    0.04914          0        106        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.06838     0.0499          0         83        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06568    0.05048          0         84        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06319    0.04898          0         71        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G     0.0615    0.04834          0         75        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.17it/s]\n","                   all        145        235      0.537      0.481      0.467      0.163\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m131 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.53708,              0.48085,              0.46711,              0.16274,             0.061339,             0.020591,                    0,              0.00623,              0.01131,              0.95199,              0.00049,               2.7524,               0.7875,              0.08781,              0.05365,              0.47701,               1.1859,               1.3076,              0.95253,                  0.2,               4.0012,                    0,              0.01408,              0.75843,              0.48478,                    0,              0.09203,              0.50466,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01, momentum=0.96896, weight_decay=0.00054, warmup_epochs=2.54324, warmup_momentum=0.78895, warmup_bias_lr=0.08324, box=0.05687, cls=0.4236, cls_pw=1.31146, obj=1.19235, obj_pw=1.003, iou_t=0.2, anchor_t=3.91185, fl_gamma=0.0, hsv_h=0.01455, hsv_s=0.76917, hsv_v=0.47231, degrees=0.0, translate=0.08837, scale=0.53196, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00054), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 191.41it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 600.20it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 1.0000 best possible recall, 4.99 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.513-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1229    0.03978          0         80        320: 100% 28/28 [00:07<00:00,  3.65it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1052    0.04835          0         76        320: 100% 28/28 [00:07<00:00,  3.94it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09422    0.04931          0         71        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08858    0.04984          0         87        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08142    0.04854          0         88        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07641    0.04656          0        108        320: 100% 28/28 [00:08<00:00,  3.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07181    0.04745          0         83        320: 100% 28/28 [00:06<00:00,  4.01it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06972    0.04772          0         86        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06743    0.04602          0         71        320: 100% 28/28 [00:06<00:00,  4.10it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06564    0.04586          0         74        320: 100% 28/28 [00:06<00:00,  4.10it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.51it/s]\n","                   all        145        235      0.563      0.426      0.441      0.145\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m132 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.56332,              0.42553,              0.44087,              0.14546,             0.067143,             0.019504,                    0,              0.00636,                 0.01,              0.96896,              0.00054,               2.5432,              0.78895,              0.08324,              0.05687,               0.4236,               1.3115,               1.1924,                1.003,                  0.2,               3.9118,                    0,              0.01455,              0.76917,              0.47231,                    0,              0.08837,              0.53196,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00682, lrf=0.01, momentum=0.98, weight_decay=0.00065, warmup_epochs=2.7154, warmup_momentum=0.74443, warmup_bias_lr=0.08766, box=0.05023, cls=0.3109, cls_pw=0.96621, obj=1.53389, obj_pw=0.98657, iou_t=0.2, anchor_t=3.84638, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.64926, hsv_v=0.52953, degrees=0.0, translate=0.09203, scale=0.59179, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.75605\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.75605\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n","Model summary: 214 layers, 7022326 parameters, 7022326 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00682) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00065), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 184.01it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.02 anchors/target, 0.005 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7798: 100% 1000/1000 [00:01<00:00, 526.28it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 1.0000 best possible recall, 7.18 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=320, metric_all=0.444/0.780-mean/best, past_thr=0.508-mean: 30,25, 53,36, 57,66, 95,49, 158,42, 79,99, 130,82, 129,145, 219,101\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.85G     0.1127     0.0437          0         80        320: 100% 28/28 [00:07<00:00,  3.68it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.85G    0.09604    0.05497          0         78        320: 100% 28/28 [00:07<00:00,  3.89it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.85G    0.08421    0.05826          0         72        320: 100% 28/28 [00:07<00:00,  3.99it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.85G    0.07782    0.05902          0         90        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.85G    0.07242    0.05743          0         93        320: 100% 28/28 [00:06<00:00,  4.01it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.85G    0.06677    0.05541          0        112        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.85G    0.06351    0.05629          0         81        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.85G    0.06156    0.05687          0         91        320: 100% 28/28 [00:07<00:00,  3.95it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.85G    0.05987    0.05518          0         72        320: 100% 28/28 [00:07<00:00,  3.96it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.85G    0.05858     0.0549          0         74        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.91it/s]\n","                   all        145        235       0.54      0.472      0.439      0.139\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m133 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.54013,              0.47234,              0.43911,              0.13858,             0.058197,             0.026081,                    0,              0.00682,                 0.01,                 0.98,              0.00065,               2.7154,              0.74443,              0.08766,              0.05023,               0.3109,              0.96621,               1.5339,              0.98657,                  0.2,               3.8464,                    0,              0.01413,              0.64926,              0.52953,                    0,              0.09203,              0.59179,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.7561\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00671, lrf=0.01051, momentum=0.97123, weight_decay=0.0005, warmup_epochs=2.38939, warmup_momentum=0.7522, warmup_bias_lr=0.09121, box=0.06374, cls=0.47565, cls_pw=1.30168, obj=1.29317, obj_pw=0.9587, iou_t=0.2, anchor_t=3.7831, fl_gamma=0.0, hsv_h=0.01463, hsv_s=0.68951, hsv_v=0.42725, degrees=0.0, translate=0.09177, scale=0.49717, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00671) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 186.75it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 574.10it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 1.0000 best possible recall, 4.92 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.517-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1366    0.04141          0         80        320: 100% 28/28 [00:07<00:00,  3.68it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1161    0.05037          0         75        320: 100% 28/28 [00:07<00:00,  3.94it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G      0.104    0.05118          0         71        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09792    0.05143          0         85        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.09001    0.05027          0         88        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08404    0.04829          0        106        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07982    0.04912          0         83        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07761    0.04953          0         83        320: 100% 28/28 [00:08<00:00,  3.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07434    0.04776          0         71        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07228    0.04727          0         75        320: 100% 28/28 [00:06<00:00,  4.01it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.18it/s]\n","                   all        145        235      0.544      0.426      0.415      0.142\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m134 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.54421,              0.42553,              0.41506,              0.14224,              0.07386,              0.02061,                    0,              0.00671,              0.01051,              0.97123,               0.0005,               2.3894,               0.7522,              0.09121,              0.06374,              0.47565,               1.3017,               1.2932,               0.9587,                  0.2,               3.7831,                    0,              0.01463,              0.68951,              0.42725,                    0,              0.09177,              0.49717,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01055, momentum=0.97122, weight_decay=0.00046, warmup_epochs=2.67347, warmup_momentum=0.81531, warmup_bias_lr=0.0903, box=0.05767, cls=0.45581, cls_pw=1.2681, obj=1.26784, obj_pw=0.95424, iou_t=0.2, anchor_t=4.0012, fl_gamma=0.0, hsv_h=0.01414, hsv_s=0.75794, hsv_v=0.43107, degrees=0.0, translate=0.0919, scale=0.50645, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.05832\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.05832\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00046), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 187.35it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:02<00:00, 423.66it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1242    0.04118          0         80        320: 100% 28/28 [00:07<00:00,  3.67it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1053    0.05028          0         75        320: 100% 28/28 [00:07<00:00,  3.86it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09459    0.05081          0         71        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08885    0.05087          0         85        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08162    0.04973          0         88        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07626    0.04803          0        106        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07263    0.04897          0         83        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07054    0.04934          0         84        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06806     0.0477          0         71        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06607    0.04706          0         75        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.46it/s]\n","                   all        145        235      0.566      0.404      0.422      0.146\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m135 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.56575,              0.40426,              0.42217,              0.14614,              0.06825,             0.020178,                    0,              0.00636,              0.01055,              0.97122,              0.00046,               2.6735,              0.81531,               0.0903,              0.05767,              0.45581,               1.2681,               1.2678,              0.95424,                  0.2,               4.0012,                    0,              0.01414,              0.75794,              0.43107,                    0,               0.0919,              0.50645,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.0583\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00653, lrf=0.01055, momentum=0.9568, weight_decay=0.00048, warmup_epochs=2.77323, warmup_momentum=0.78895, warmup_bias_lr=0.08766, box=0.05867, cls=0.4403, cls_pw=1.18785, obj=1.2945, obj_pw=1.08708, iou_t=0.2, anchor_t=3.89117, fl_gamma=0.0, hsv_h=0.0138, hsv_s=0.75515, hsv_v=0.47737, degrees=0.0, translate=0.09007, scale=0.51921, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00653) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 192.11it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 590.35it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.26: 1.0000 best possible recall, 4.98 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.514-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1264    0.04628          0         79        320: 100% 28/28 [00:07<00:00,  3.62it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1074    0.05599          0         75        320: 100% 28/28 [00:07<00:00,  3.92it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09669    0.05616          0         71        320: 100% 28/28 [00:07<00:00,  3.96it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09115     0.0565          0         86        320: 100% 28/28 [00:07<00:00,  3.97it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08342    0.05504          0         88        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G     0.0774    0.05323          0        107        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07395     0.0537          0         83        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07156    0.05441          0         84        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06857    0.05242          0         71        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06666    0.05214          0         75        320: 100% 28/28 [00:08<00:00,  3.22it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.18it/s]\n","                   all        145        235      0.603       0.46       0.47      0.171\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m136 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.60324,              0.45957,              0.46953,              0.17121,             0.068445,             0.021693,                    0,              0.00653,              0.01055,               0.9568,              0.00048,               2.7732,              0.78895,              0.08766,              0.05867,               0.4403,               1.1879,               1.2945,               1.0871,                  0.2,               3.8912,                    0,               0.0138,              0.75515,              0.47737,                    0,              0.09007,              0.51921,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00637, lrf=0.01, momentum=0.96301, weight_decay=0.00056, warmup_epochs=2.7154, warmup_momentum=0.95, warmup_bias_lr=0.11104, box=0.03305, cls=0.78275, cls_pw=0.84254, obj=1.63716, obj_pw=0.98657, iou_t=0.2, anchor_t=4.94699, fl_gamma=0.0, hsv_h=0.016, hsv_s=0.77425, hsv_v=0.4228, degrees=0.0, translate=0.0814, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=3.03014\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=3.03014\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512]]\n","Model summary: 214 layers, 7022326 parameters, 7022326 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00637) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00056), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 191.65it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.06 anchors/target, 0.019 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7798: 100% 1000/1000 [00:02<00:00, 499.66it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.20: 1.0000 best possible recall, 8.02 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=320, metric_all=0.444/0.780-mean/best, past_thr=0.479-mean: 30,25, 53,36, 57,66, 95,49, 158,42, 79,99, 130,82, 129,145, 219,101\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.85G    0.07481    0.05094          0         80        320: 100% 28/28 [00:07<00:00,  3.51it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.85G    0.06329    0.06309          0         75        320: 100% 28/28 [00:08<00:00,  3.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.85G    0.05564    0.06601          0         71        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.85G    0.05167    0.06768          0         84        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.85G    0.04756    0.06628          0         88        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.85G    0.04494    0.06365          0        107        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.85G    0.04293    0.06475          0         84        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.85G    0.04179    0.06554          0         83        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.85G     0.0401    0.06375          0         71        320: 100% 28/28 [00:06<00:00,  4.07it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.85G    0.03908    0.06281          0         75        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.34it/s]\n","                   all        145        235      0.551      0.477      0.449      0.155\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m137 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.55098,               0.4766,              0.44937,              0.15473,             0.039009,             0.027993,                    0,              0.00637,                 0.01,              0.96301,              0.00056,               2.7154,                 0.95,              0.11104,              0.03305,              0.78275,              0.84254,               1.6372,              0.98657,                  0.2,                4.947,                    0,                0.016,              0.77425,               0.4228,                    0,               0.0814,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               3.0301\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00599, lrf=0.01, momentum=0.96301, weight_decay=0.00048, warmup_epochs=2.81103, warmup_momentum=0.78474, warmup_bias_lr=0.10124, box=0.06578, cls=0.46089, cls_pw=1.2201, obj=1.2945, obj_pw=1.14767, iou_t=0.2, anchor_t=4.15138, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.6656, hsv_v=0.49719, degrees=0.0, translate=0.1017, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.9458, mixup=0.0, copy_paste=0.0, anchors=2.34905\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.34905\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00599) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 192.68it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 523.70it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 1.0000 best possible recall, 5.13 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.506-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1428    0.04763          0         80        320: 100% 28/28 [00:07<00:00,  3.69it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1217    0.05776          0         79        320: 100% 28/28 [00:07<00:00,  3.94it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1087    0.05875          0         84        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G     0.1025    0.05748          0         78        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.09417    0.05619          0         96        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08886     0.0562          0        108        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.08366     0.0563          0         85        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.08024    0.05508          0         86        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07865    0.05495          0         83        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G     0.0762    0.05408          0         88        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.20it/s]\n","                   all        145        235      0.503      0.404      0.402      0.147\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m138 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.50312,              0.40426,              0.40199,              0.14698,             0.076804,             0.024982,                    0,              0.00599,                 0.01,              0.96301,              0.00048,                2.811,              0.78474,              0.10124,              0.06578,              0.46089,               1.2201,               1.2945,               1.1477,                  0.2,               4.1514,                    0,              0.01413,               0.6656,              0.49719,                    0,               0.1017,              0.49617,                    0,                    0,                    0,                  0.5,               0.9458,                    0,                    0,               2.3491\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01055, momentum=0.98, weight_decay=0.00045, warmup_epochs=2.7154, warmup_momentum=0.86818, warmup_bias_lr=0.07687, box=0.07219, cls=0.55009, cls_pw=1.13539, obj=1.36074, obj_pw=1.0116, iou_t=0.2, anchor_t=5.1783, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.9, hsv_v=0.48899, degrees=0.0, translate=0.08879, scale=0.50998, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.45163\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.45163\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00045), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:05<00:00, 149.99it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 626.17it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.19: 1.0000 best possible recall, 5.56 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.484-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1571     0.0499          0         79        320: 100% 28/28 [00:07<00:00,  3.71it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1317    0.06087          0         75        320: 100% 28/28 [00:07<00:00,  3.89it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1197    0.06076          0         71        320: 100% 28/28 [00:07<00:00,  4.00it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G     0.1129    0.06152          0         84        320: 100% 28/28 [00:08<00:00,  3.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G     0.1052    0.06006          0         88        320: 100% 28/28 [00:07<00:00,  3.98it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.09965    0.05869          0        107        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.09609    0.05929          0         83        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.09362    0.06011          0         84        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.09073    0.05801          0         71        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G     0.0888    0.05763          0         75        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.42it/s]\n","                   all        145        235      0.509      0.417      0.405      0.146\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m139 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.50898,              0.41702,              0.40536,              0.14559,              0.08755,             0.025267,                    0,              0.00636,              0.01055,                 0.98,              0.00045,               2.7154,              0.86818,              0.07687,              0.07219,              0.55009,               1.1354,               1.3607,               1.0116,                  0.2,               5.1783,                    0,              0.01413,                  0.9,              0.48899,                    0,              0.08879,              0.50998,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.4516\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01052, momentum=0.96301, weight_decay=0.00049, warmup_epochs=2.71229, warmup_momentum=0.79128, warmup_bias_lr=0.08839, box=0.05833, cls=0.45891, cls_pw=1.23534, obj=1.29564, obj_pw=0.97995, iou_t=0.2, anchor_t=3.98365, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.73683, hsv_v=0.47936, degrees=0.0, translate=0.09226, scale=0.49461, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00049), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 193.53it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 546.29it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.03 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1257    0.04287          0         80        320: 100% 28/28 [00:07<00:00,  3.66it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1072    0.05187          0         75        320: 100% 28/28 [00:07<00:00,  3.98it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09608    0.05285          0         71        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09051    0.05308          0         85        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08332    0.05189          0         88        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07769    0.05011          0        107        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07394    0.05073          0         83        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07108    0.05147          0         82        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06847    0.04944          0         71        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06699    0.04896          0         75        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.31it/s]\n","                   all        145        235        0.6      0.485      0.478      0.167\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m140 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.60003,              0.48517,              0.47813,              0.16731,             0.066474,             0.020778,                    0,              0.00636,              0.01052,              0.96301,              0.00049,               2.7123,              0.79128,              0.08839,              0.05833,              0.45891,               1.2353,               1.2956,              0.97995,                  0.2,               3.9836,                    0,              0.01413,              0.73683,              0.47936,                    0,              0.09226,              0.49461,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00563, lrf=0.01055, momentum=0.94643, weight_decay=0.0005, warmup_epochs=2.09357, warmup_momentum=0.75171, warmup_bias_lr=0.10079, box=0.08422, cls=0.48245, cls_pw=1.2201, obj=0.96874, obj_pw=0.72857, iou_t=0.2, anchor_t=3.64885, fl_gamma=0.0, hsv_h=0.01344, hsv_s=0.74193, hsv_v=0.39531, degrees=0.0, translate=0.10597, scale=0.3562, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00563) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 192.96it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7513: 100% 1000/1000 [00:01<00:00, 539.05it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.27: 1.0000 best possible recall, 4.77 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.459/0.751-mean/best, past_thr=0.524-mean: 31,24, 51,40, 105,44, 67,77, 133,81, 111,142\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1782    0.02423          0         75        320: 100% 28/28 [00:07<00:00,  3.61it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1517    0.02983          0         70        320: 100% 28/28 [00:08<00:00,  3.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1364    0.03202          0         66        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G     0.1277    0.03293          0         75        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G     0.1166    0.03215          0         85        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G     0.1099    0.03061          0         93        320: 100% 28/28 [00:08<00:00,  3.46it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G     0.1044    0.03094          0         79        320: 100% 28/28 [00:07<00:00,  3.89it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G     0.1008    0.03069          0         75        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G     0.0968    0.02952          0         71        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.09361    0.02937          0         73        320: 100% 28/28 [00:06<00:00,  4.04it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.19it/s]\n","                   all        145        235      0.377      0.374        0.3      0.101\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m141 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.37744,              0.37447,               0.2999,              0.10093,              0.10524,             0.012811,                    0,              0.00563,              0.01055,              0.94643,               0.0005,               2.0936,              0.75171,              0.10079,              0.08422,              0.48245,               1.2201,              0.96874,              0.72857,                  0.2,               3.6488,                    0,              0.01344,              0.74193,              0.39531,                    0,              0.10597,               0.3562,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00762, lrf=0.01, momentum=0.96301, weight_decay=0.00061, warmup_epochs=2.24389, warmup_momentum=0.70208, warmup_bias_lr=0.08766, box=0.06182, cls=0.45884, cls_pw=1.3983, obj=1.2945, obj_pw=1.12874, iou_t=0.2, anchor_t=3.97234, fl_gamma=0.0, hsv_h=0.01413, hsv_s=0.79294, hsv_v=0.47057, degrees=0.0, translate=0.09203, scale=0.44698, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00762) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00061), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 190.53it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 557.14it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.02 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.512-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1329     0.0479          0         79        320: 100% 28/28 [00:07<00:00,  3.83it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1139    0.05742          0         74        320: 100% 28/28 [00:07<00:00,  3.93it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1021    0.05865          0         69        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09661    0.05821          0         81        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08787    0.05729          0         86        320: 100% 28/28 [00:06<00:00,  4.09it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G     0.0817    0.05519          0        102        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07771    0.05656          0         84        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07504    0.05669          0         82        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07212    0.05446          0         73        320: 100% 28/28 [00:06<00:00,  4.27it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07005    0.05406          0         75        320: 100% 28/28 [00:06<00:00,  4.29it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.23it/s]\n","                   all        145        235      0.619      0.457      0.472      0.159\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m142 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.61911,              0.45652,              0.47181,              0.15871,             0.071767,             0.022694,                    0,              0.00762,                 0.01,              0.96301,              0.00061,               2.2439,              0.70208,              0.08766,              0.06182,              0.45884,               1.3983,               1.2945,               1.1287,                  0.2,               3.9723,                    0,              0.01413,              0.79294,              0.47057,                    0,              0.09203,              0.44698,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00583, lrf=0.0112, momentum=0.96638, weight_decay=0.0005, warmup_epochs=2.68521, warmup_momentum=0.72757, warmup_bias_lr=0.08766, box=0.06226, cls=0.47249, cls_pw=1.2302, obj=1.2945, obj_pw=1.01115, iou_t=0.2, anchor_t=4.00818, fl_gamma=0.0, hsv_h=0.01512, hsv_s=0.87439, hsv_v=0.462, degrees=0.0, translate=0.08824, scale=0.52199, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00583) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 185.10it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 636.83it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1357    0.04332          0         79        320: 100% 28/28 [00:07<00:00,  3.66it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1175     0.0525          0         75        320: 100% 28/28 [00:06<00:00,  4.06it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1048    0.05442          0         71        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09811    0.05531          0         87        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.09042    0.05417          0         88        320: 100% 28/28 [00:08<00:00,  3.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.08437     0.0515          0        107        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.08045    0.05208          0         83        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07752    0.05288          0         84        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07475    0.05078          0         71        320: 100% 28/28 [00:07<00:00,  3.79it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07272    0.05078          0         75        320: 100% 28/28 [00:07<00:00,  3.52it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.60it/s]\n","                   all        145        235      0.545       0.47      0.448       0.14\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m143 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.54538,              0.46965,              0.44846,              0.13988,              0.07426,             0.021305,                    0,              0.00583,               0.0112,              0.96638,               0.0005,               2.6852,              0.72757,              0.08766,              0.06226,              0.47249,               1.2302,               1.2945,               1.0111,                  0.2,               4.0082,                    0,              0.01512,              0.87439,                0.462,                    0,              0.08824,              0.52199,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00635, lrf=0.01, momentum=0.98, weight_decay=0.00048, warmup_epochs=2.7154, warmup_momentum=0.78895, warmup_bias_lr=0.08766, box=0.05634, cls=0.49871, cls_pw=1.245, obj=1.2945, obj_pw=0.98657, iou_t=0.2, anchor_t=3.6789, fl_gamma=0.0, hsv_h=0.01081, hsv_s=0.59221, hsv_v=0.52172, degrees=0.0, translate=0.08978, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00635) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 193.46it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7499: 100% 1000/1000 [00:01<00:00, 584.32it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.27: 1.0000 best possible recall, 4.82 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.523-mean: 30,26, 57,36, 59,69, 120,52, 94,100, 157,120\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1205    0.04189          0         80        320: 100% 28/28 [00:07<00:00,  3.80it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1028    0.05069          0         75        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09215    0.05124          0         71        320: 100% 28/28 [00:06<00:00,  4.20it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08625    0.05167          0         85        320: 100% 28/28 [00:06<00:00,  4.33it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.07908     0.0506          0         88        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07459     0.0485          0        106        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07038    0.04977          0         83        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06856    0.04986          0         83        320: 100% 28/28 [00:06<00:00,  4.15it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06624    0.04806          0         70        320: 100% 28/28 [00:06<00:00,  4.35it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06467    0.04777          0         75        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.30it/s]\n","                   all        145        235       0.54      0.429      0.413      0.137\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m144 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.53956,              0.42885,              0.41318,              0.13724,              0.06632,             0.020717,                    0,              0.00635,                 0.01,                 0.98,              0.00048,               2.7154,              0.78895,              0.08766,              0.05634,              0.49871,                1.245,               1.2945,              0.98657,                  0.2,               3.6789,                    0,              0.01081,              0.59221,              0.52172,                    0,              0.08978,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01, momentum=0.95772, weight_decay=0.00052, warmup_epochs=2.7154, warmup_momentum=0.78269, warmup_bias_lr=0.08766, box=0.0691, cls=0.41961, cls_pw=1.13168, obj=1.51562, obj_pw=1.05077, iou_t=0.2, anchor_t=4.17269, fl_gamma=0.0, hsv_h=0.01605, hsv_s=0.7391, hsv_v=0.47057, degrees=0.0, translate=0.0822, scale=0.4709, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00052), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 195.43it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 667.36it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 1.0000 best possible recall, 5.14 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.506-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1485    0.05419          0         80        320: 100% 28/28 [00:07<00:00,  3.83it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1263    0.06515          0         74        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1135    0.06594          0         70        320: 100% 28/28 [00:06<00:00,  4.31it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G     0.1075     0.0663          0         82        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.09862     0.0648          0         88        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.09243    0.06223          0        103        320: 100% 28/28 [00:06<00:00,  4.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.08731    0.06338          0         84        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.08468    0.06403          0         82        320: 100% 28/28 [00:06<00:00,  4.17it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.08122    0.06184          0         73        320: 100% 28/28 [00:08<00:00,  3.27it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.07927    0.06105          0         75        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.39it/s]\n","                   all        145        235      0.497       0.46      0.418      0.152\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m145 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.49702,              0.45957,              0.41765,              0.15173,             0.082416,             0.025472,                    0,              0.00636,                 0.01,              0.95772,              0.00052,               2.7154,              0.78269,              0.08766,               0.0691,              0.41961,               1.1317,               1.5156,               1.0508,                  0.2,               4.1727,                    0,              0.01605,               0.7391,              0.47057,                    0,               0.0822,               0.4709,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00579, lrf=0.01055, momentum=0.96301, weight_decay=0.00052, warmup_epochs=2.6512, warmup_momentum=0.82651, warmup_bias_lr=0.09415, box=0.05879, cls=0.46089, cls_pw=1.40999, obj=1.49526, obj_pw=1.05258, iou_t=0.2, anchor_t=4.00038, fl_gamma=0.0, hsv_h=0.01336, hsv_s=0.83372, hsv_v=0.48127, degrees=0.0, translate=0.09024, scale=0.37615, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.39548\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.39548\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00579) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00052), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 194.56it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 564.13it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.04 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1253    0.05228          0         77        320: 100% 28/28 [00:07<00:00,  3.81it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1063     0.0625          0         72        320: 100% 28/28 [00:08<00:00,  3.26it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.0954    0.06371          0         68        320: 100% 28/28 [00:06<00:00,  4.31it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08958    0.06358          0         77        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08173    0.06252          0         84        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07685    0.05992          0         93        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07333    0.06158          0         82        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07069    0.06167          0         78        320: 100% 28/28 [00:06<00:00,  4.29it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.06795    0.05928          0         71        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06655    0.05861          0         74        320: 100% 28/28 [00:06<00:00,  4.28it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.40it/s]\n","                   all        145        235      0.586      0.446      0.472      0.159\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m146 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.58622,              0.44612,              0.47188,              0.15854,             0.068449,             0.025529,                    0,              0.00579,              0.01055,              0.96301,              0.00052,               2.6512,              0.82651,              0.09415,              0.05879,              0.46089,                 1.41,               1.4953,               1.0526,                  0.2,               4.0004,                    0,              0.01336,              0.83372,              0.48127,                    0,              0.09024,              0.37615,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,               2.3955\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01453, momentum=0.98, weight_decay=0.00036, warmup_epochs=2.7154, warmup_momentum=0.87454, warmup_bias_lr=0.09044, box=0.07376, cls=0.39234, cls_pw=1.43197, obj=1.46778, obj_pw=0.92486, iou_t=0.2, anchor_t=4.15804, fl_gamma=0.0, hsv_h=0.01626, hsv_s=0.79853, hsv_v=0.47057, degrees=0.0, translate=0.07384, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00036), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 195.30it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 646.41it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.24: 1.0000 best possible recall, 5.13 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.506-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1567    0.04822          0         80        320: 100% 28/28 [00:07<00:00,  3.77it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1295    0.05913          0         74        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G     0.1181    0.05819          0         71        320: 100% 28/28 [00:06<00:00,  4.08it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G      0.111    0.05911          0         84        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G     0.1028    0.05771          0         88        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.09642    0.05542          0        107        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.09318    0.05667          0         84        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.09119    0.05718          0         84        320: 100% 28/28 [00:06<00:00,  4.22it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.08792    0.05554          0         71        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.08605    0.05489          0         74        320: 100% 28/28 [00:06<00:00,  4.16it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.26it/s]\n","                   all        145        235      0.559      0.434       0.42      0.142\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m147 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.55884,              0.43404,              0.42003,              0.14221,             0.085482,             0.023205,                    0,              0.00636,              0.01453,                 0.98,              0.00036,               2.7154,              0.87454,              0.09044,              0.07376,              0.39234,                1.432,               1.4678,              0.92486,                  0.2,                4.158,                    0,              0.01626,              0.79853,              0.47057,                    0,              0.07384,              0.49617,                    0,                    0,                    0,                  0.5,                    1,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0063, lrf=0.01055, momentum=0.96241, weight_decay=0.00048, warmup_epochs=2.74658, warmup_momentum=0.78895, warmup_bias_lr=0.08891, box=0.05916, cls=0.45977, cls_pw=1.22142, obj=1.30342, obj_pw=0.98528, iou_t=0.2, anchor_t=3.97939, fl_gamma=0.0, hsv_h=0.01437, hsv_s=0.73137, hsv_v=0.47119, degrees=0.0, translate=0.09259, scale=0.49705, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.99219, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0063) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00048), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 196.89it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 615.77it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.03 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.511-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1271    0.04284          0         76        320: 100% 28/28 [00:09<00:00,  3.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1091    0.05208          0         75        320: 100% 28/28 [00:07<00:00,  3.89it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09742    0.05357          0        109        320: 100% 28/28 [00:06<00:00,  4.12it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.09169    0.05297          0         88        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.08458    0.05207          0         92        320: 100% 28/28 [00:06<00:00,  4.14it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.07888    0.04986          0        104        320: 100% 28/28 [00:06<00:00,  4.03it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07505    0.05049          0         82        320: 100% 28/28 [00:08<00:00,  3.46it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.07163    0.05013          0         58        320: 100% 28/28 [00:06<00:00,  4.29it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.07029    0.04974          0         85        320: 100% 28/28 [00:06<00:00,  4.21it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.06718     0.0495          0         67        320: 100% 28/28 [00:06<00:00,  4.13it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.24it/s]\n","                   all        145        235      0.482       0.46       0.41      0.133\n","\n","10 epochs completed in 0.020 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m148 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.48195,              0.45957,              0.41041,              0.13298,             0.070717,             0.022264,                    0,               0.0063,              0.01055,              0.96241,              0.00048,               2.7466,              0.78895,              0.08891,              0.05916,              0.45977,               1.2214,               1.3034,              0.98528,                  0.2,               3.9794,                    0,              0.01437,              0.73137,              0.47119,                    0,              0.09259,              0.49705,                    0,                    0,                    0,                  0.5,              0.99219,                    0,                    0,                    2\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00636, lrf=0.01219, momentum=0.98, weight_decay=0.00044, warmup_epochs=2.43087, warmup_momentum=0.79214, warmup_bias_lr=0.09281, box=0.07049, cls=0.38969, cls_pw=1.2201, obj=1.32107, obj_pw=0.99339, iou_t=0.2, anchor_t=4.71155, fl_gamma=0.0, hsv_h=0.01152, hsv_s=0.7391, hsv_v=0.48238, degrees=0.0, translate=0.08343, scale=0.49617, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.97192, mixup=0.0, copy_paste=0.0, anchors=2.17349\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.17349\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00636) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00044), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 194.31it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:02<00:00, 497.96it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.21: 1.0000 best possible recall, 5.40 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.492-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1536    0.04551          0         76        320: 100% 28/28 [00:07<00:00,  3.93it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1307    0.05608          0         78        320: 100% 28/28 [00:06<00:00,  4.02it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G      0.117    0.05643          0         89        320: 100% 28/28 [00:06<00:00,  4.18it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G     0.1112    0.05479          0         80        320: 100% 28/28 [00:06<00:00,  4.33it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G     0.1038    0.05456          0         82        320: 100% 28/28 [00:06<00:00,  4.24it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G    0.09559    0.05521          0         81        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.09228    0.05498          0         94        320: 100% 28/28 [00:06<00:00,  4.30it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G     0.0899    0.05472          0         77        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        8/9      1.84G    0.08739    0.05432          0         85        320: 100% 28/28 [00:06<00:00,  4.27it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        9/9      1.84G    0.08567    0.05385          0         80        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  2.29it/s]\n","                   all        145        235      0.548      0.426       0.45      0.145\n","\n","10 epochs completed in 0.019 hours.\n","Results saved to \u001b[1mruns/evolve/exp7\u001b[0m\n","\u001b[34m\u001b[1mevolve: \u001b[0m149 generations finished, current result:\n","\u001b[34m\u001b[1mevolve: \u001b[0m   metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss,                  lr0,                  lrf,             momentum,         weight_decay,        warmup_epochs,      warmup_momentum,       warmup_bias_lr,                  box,                  cls,               cls_pw,                  obj,               obj_pw,                iou_t,             anchor_t,             fl_gamma,                hsv_h,                hsv_s,                hsv_v,              degrees,            translate,                scale,                shear,          perspective,               flipud,               fliplr,               mosaic,                mixup,           copy_paste,              anchors\n","\u001b[34m\u001b[1mevolve: \u001b[0m             0.54818,              0.42553,              0.45013,              0.14506,             0.085193,              0.02325,                    0,              0.00636,              0.01219,                 0.98,              0.00044,               2.4309,              0.79214,              0.09281,              0.07049,              0.38969,               1.2201,               1.3211,              0.99339,                  0.2,               4.7115,                    0,              0.01152,               0.7391,              0.48238,                    0,              0.08343,              0.49617,                    0,                    0,                    0,                  0.5,              0.97192,                    0,                    0,               2.1735\n","\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00599, lrf=0.01114, momentum=0.96319, weight_decay=0.00051, warmup_epochs=2.7154, warmup_momentum=0.80143, warmup_bias_lr=0.08766, box=0.05516, cls=0.46089, cls_pw=1.19218, obj=1.31914, obj_pw=0.89844, iou_t=0.2, anchor_t=3.92668, fl_gamma=0.0, hsv_h=0.0133, hsv_s=0.82216, hsv_v=0.49573, degrees=0.0, translate=0.09203, scale=0.54894, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, anchors=2.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.00599) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00051), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 886/886 [00:04<00:00, 191.82it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7498: 100% 1000/1000 [00:01<00:00, 603.47it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.00 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.460/0.750-mean/best, past_thr=0.513-mean: 31,26, 59,37, 56,72, 120,52, 94,100, 158,121\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/evolve/exp7/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/evolve/exp7\u001b[0m\n","Starting training for 10 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        0/9      1.84G     0.1195    0.03996          0         80        320: 100% 28/28 [00:07<00:00,  3.82it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        1/9      1.84G     0.1029    0.04845          0         77        320: 100% 28/28 [00:06<00:00,  4.05it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        2/9      1.84G    0.09214    0.04972          0         71        320: 100% 28/28 [00:06<00:00,  4.19it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        3/9      1.84G    0.08681    0.05019          0         88        320: 100% 28/28 [00:06<00:00,  4.11it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        4/9      1.84G    0.07936    0.04927          0         90        320: 100% 28/28 [00:06<00:00,  4.23it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        5/9      1.84G     0.0744    0.04737          0        108        320: 100% 28/28 [00:06<00:00,  4.25it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        6/9      1.84G    0.07025    0.04815          0         83        320: 100% 28/28 [00:08<00:00,  3.34it/s]\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","        7/9      1.84G    0.06792    0.04873          0        132        320:  96% 27/28 [00:06<00:00,  4.43it/s]"]}],"source":["!cd yolov5 && python train.py --img 320 --batch 32 --epochs 10 --data carScr_up.yaml --weights yolov5s.pt --cache --evolve"]},{"cell_type":"code","execution_count":5,"id":"hMgBKciTVuRN","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1360326,"status":"ok","timestamp":1672943875085,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"hMgBKciTVuRN","outputId":"bad55b45-3ddf-4af1-bfe9-e38a79794cfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"gitpython\" not found, attempting AutoUpdate...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gitpython\n","  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.0/184.0 KB 7.5 MB/s eta 0:00:00\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 KB 8.2 MB/s eta 0:00:00\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, gitdb, gitpython\n","Successfully installed gitdb-4.0.10 gitpython-3.1.30 smmap-5.0.0\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['gitpython']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=carScr_up.yaml, hyp=/content/drive/MyDrive/Zummit/Zummit/yolov5/runs/evolve/exp7/hyp_evolve.yaml, epochs=50, batch_size=32, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","Command 'git fetch origin' timed out after 5 seconds\n","\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirements \"ipython\" \"thop>=0.1.1\" not found, attempting AutoUpdate...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (7.9.0)\n","Collecting thop>=0.1.1\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython) (57.4.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython) (2.6.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython) (4.4.2)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython) (2.0.10)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 27.1 MB/s eta 0:00:00\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython) (5.7.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython) (4.8.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython) (0.2.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython) (0.7.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from thop>=0.1.1) (1.13.0+cu116)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython) (0.8.3)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython) (1.15.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython) (0.7.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->thop>=0.1.1) (4.4.0)\n","Installing collected packages: jedi, thop\n","Successfully installed jedi-0.18.2 thop-0.1.1.post2209072238\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 2 packages updated per /content/drive/MyDrive/Zummit/Zummit/yolov5/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 🚀 v7.0-21-ga1b6e79 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01018, lrf=0.01, momentum=0.91055, weight_decay=0.00047, warmup_epochs=2.371, warmup_momentum=0.95, warmup_bias_lr=0.09791, box=0.03418, cls=0.41403, cls_pw=1.1871, obj=1.1892, obj_pw=0.94252, iou_t=0.2, anchor_t=4.0464, fl_gamma=0.0, hsv_h=0.01357, hsv_s=0.76975, hsv_v=0.44424, degrees=0.0, translate=0.08478, scale=0.49294, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.94748, mixup=0.0, copy_paste=0.0, anchors=2.0406\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 23.6MB/s]\n","Overriding model.yaml nc=80 with nc=1\n","Overriding model.yaml anchors with anchors=2.0406\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     10788  models.yolo.Detect                      [1, [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], [128, 256, 512]]\n","Model summary: 214 layers, 7016932 parameters, 7016932 gradients\n","\n","Transferred 342/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01018) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.00047), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Dataset.cache... 886 images, 0 backgrounds, 0 corrupt: 100% 886/886 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Zummit/Zummit/Dataset_up/labels/Images1.cache... 145 images, 0 backgrounds, 0 corrupt: 100% 145/145 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m0.00 anchors/target, 0.000 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 6 anchors on 1749 points...\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7514: 100% 1000/1000 [00:01<00:00, 790.75it/s]\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 5.05 anchors past thr\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=6, img_size=320, metric_all=0.459/0.751-mean/best, past_thr=0.511-mean: 32,26, 54,41, 108,44, 67,79, 135,81, 123,144\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n","Plotting labels to runs/train/exp32/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/train/exp32\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       0/49      1.87G    0.07303    0.03705          0         75        320: 100% 28/28 [03:10<00:00,  6.82s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.29it/s]\n","                   all        145        235     0.0176     0.0936      0.011    0.00285\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       1/49      2.37G    0.05995    0.04546          0         82        320: 100% 28/28 [00:18<00:00,  1.50it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.40it/s]\n","                   all        145        235      0.097      0.115     0.0473     0.0136\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       2/49      2.37G    0.05396    0.04421          0         91        320: 100% 28/28 [00:21<00:00,  1.33it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.40it/s]\n","                   all        145        235      0.202      0.255      0.151     0.0347\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       3/49      2.37G    0.05189    0.04298          0         79        320: 100% 28/28 [00:19<00:00,  1.40it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.17it/s]\n","                   all        145        235      0.261      0.255      0.189     0.0475\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       4/49      2.37G    0.04721     0.0426          0         98        320: 100% 28/28 [00:16<00:00,  1.70it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.43it/s]\n","                   all        145        235      0.355      0.323      0.269     0.0812\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       5/49      2.37G    0.04437    0.04336          0        114        320: 100% 28/28 [00:18<00:00,  1.52it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.45it/s]\n","                   all        145        235      0.335      0.396      0.268     0.0871\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       6/49      2.37G    0.04253    0.04286          0         77        320: 100% 28/28 [00:18<00:00,  1.53it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.16it/s]\n","                   all        145        235       0.53      0.404      0.378      0.125\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       7/49      2.37G    0.04126    0.04229          0         90        320: 100% 28/28 [00:16<00:00,  1.70it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.34it/s]\n","                   all        145        235      0.479      0.494      0.416      0.131\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       8/49      2.37G    0.04006    0.04224          0         86        320: 100% 28/28 [00:18<00:00,  1.48it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.49it/s]\n","                   all        145        235      0.568      0.421      0.418      0.138\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       9/49      2.37G    0.03975    0.04133          0         76        320: 100% 28/28 [00:20<00:00,  1.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.29it/s]\n","                   all        145        235      0.587      0.481      0.474      0.154\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      10/49      2.37G    0.03943    0.04139          0         73        320: 100% 28/28 [00:18<00:00,  1.51it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.04it/s]\n","                   all        145        235      0.512      0.494      0.465      0.165\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      11/49      2.37G     0.0386    0.04046          0         73        320: 100% 28/28 [00:18<00:00,  1.51it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.36it/s]\n","                   all        145        235      0.438      0.441      0.371       0.13\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      12/49      2.37G    0.03774    0.04043          0        101        320: 100% 28/28 [00:17<00:00,  1.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.35it/s]\n","                   all        145        235      0.636      0.485      0.495      0.172\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      13/49      2.37G    0.03692    0.04066          0         88        320: 100% 28/28 [00:16<00:00,  1.70it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.40it/s]\n","                   all        145        235      0.619      0.448       0.46      0.174\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      14/49      2.37G    0.03649    0.03873          0         83        320: 100% 28/28 [00:17<00:00,  1.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.13it/s]\n","                   all        145        235      0.554      0.518      0.483      0.185\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      15/49      2.37G    0.03608    0.03947          0         75        320: 100% 28/28 [00:16<00:00,  1.74it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.34it/s]\n","                   all        145        235      0.604      0.498      0.481      0.179\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      16/49      2.37G     0.0348    0.03827          0         92        320: 100% 28/28 [00:18<00:00,  1.52it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.33it/s]\n","                   all        145        235      0.577      0.532      0.485       0.17\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      17/49      2.37G      0.035    0.03899          0         96        320: 100% 28/28 [00:19<00:00,  1.44it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.21it/s]\n","                   all        145        235      0.504       0.57      0.459      0.168\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      18/49      2.37G    0.03455    0.03823          0         78        320: 100% 28/28 [00:19<00:00,  1.47it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.34it/s]\n","                   all        145        235      0.616       0.47      0.462      0.172\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      19/49      2.37G    0.03396    0.03806          0         86        320: 100% 28/28 [00:20<00:00,  1.37it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.57it/s]\n","                   all        145        235      0.636      0.506      0.498      0.185\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      20/49      2.37G    0.03316     0.0375          0         84        320: 100% 28/28 [00:18<00:00,  1.53it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.33it/s]\n","                   all        145        235      0.632      0.511        0.5      0.181\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      21/49      2.37G    0.03295    0.03684          0         89        320: 100% 28/28 [00:17<00:00,  1.64it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.39it/s]\n","                   all        145        235      0.658      0.532      0.535      0.199\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      22/49      2.37G    0.03281    0.03747          0         63        320: 100% 28/28 [00:17<00:00,  1.59it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.42it/s]\n","                   all        145        235      0.574      0.553       0.49      0.182\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      23/49      2.37G    0.03207    0.03622          0         80        320: 100% 28/28 [00:17<00:00,  1.61it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.03it/s]\n","                   all        145        235      0.528      0.511      0.467      0.164\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      24/49      2.37G    0.03194    0.03593          0         90        320: 100% 28/28 [00:16<00:00,  1.69it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.23it/s]\n","                   all        145        235      0.582      0.472      0.443      0.164\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      25/49      2.37G    0.03103    0.03552          0         86        320: 100% 28/28 [00:18<00:00,  1.50it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.38it/s]\n","                   all        145        235      0.542      0.519      0.437      0.168\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      26/49      2.37G    0.03107    0.03588          0         91        320: 100% 28/28 [00:17<00:00,  1.64it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.35it/s]\n","                   all        145        235      0.569      0.506      0.447       0.16\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      27/49      2.37G    0.03064    0.03619          0         76        320: 100% 28/28 [00:19<00:00,  1.42it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.42it/s]\n","                   all        145        235      0.597      0.528      0.471      0.175\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      28/49      2.37G    0.03085     0.0347          0         76        320: 100% 28/28 [00:20<00:00,  1.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.43it/s]\n","                   all        145        235      0.616      0.587       0.54      0.206\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      29/49      2.37G    0.03042    0.03584          0         91        320: 100% 28/28 [00:16<00:00,  1.68it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.36it/s]\n","                   all        145        235       0.63      0.566      0.522      0.199\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      30/49      2.37G     0.0295    0.03483          0         81        320: 100% 28/28 [00:18<00:00,  1.51it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.52it/s]\n","                   all        145        235      0.555      0.596      0.499      0.195\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      31/49      2.37G     0.0296    0.03258          0         64        320: 100% 28/28 [00:17<00:00,  1.57it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.53it/s]\n","                   all        145        235      0.536      0.562      0.479      0.178\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      32/49      2.37G    0.02937    0.03509          0         99        320: 100% 28/28 [00:17<00:00,  1.64it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.48it/s]\n","                   all        145        235      0.607      0.566      0.521      0.204\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      33/49      2.37G    0.02916    0.03248          0         77        320: 100% 28/28 [00:17<00:00,  1.56it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.36it/s]\n","                   all        145        235      0.625      0.539      0.504      0.196\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      34/49      2.37G    0.02902    0.03296          0         67        320: 100% 28/28 [00:17<00:00,  1.64it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.25it/s]\n","                   all        145        235      0.567      0.545      0.496      0.172\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      35/49      2.37G    0.02845    0.03333          0         86        320: 100% 28/28 [00:18<00:00,  1.55it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.41it/s]\n","                   all        145        235      0.653      0.498      0.494      0.185\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      36/49      2.37G     0.0277    0.03282          0         71        320: 100% 28/28 [00:22<00:00,  1.23it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.51it/s]\n","                   all        145        235      0.571      0.549      0.482      0.189\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      37/49      2.37G    0.02789    0.03323          0         91        320: 100% 28/28 [00:18<00:00,  1.51it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.14it/s]\n","                   all        145        235      0.615      0.545      0.484       0.19\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      38/49      2.37G    0.02706    0.03206          0         73        320: 100% 28/28 [00:18<00:00,  1.48it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.61it/s]\n","                   all        145        235      0.595      0.549      0.479      0.178\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      39/49      2.37G    0.02767    0.03251          0         76        320: 100% 28/28 [00:19<00:00,  1.45it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.55it/s]\n","                   all        145        235      0.601      0.545      0.497       0.18\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      40/49      2.37G    0.02739     0.0314          0         66        320: 100% 28/28 [00:16<00:00,  1.69it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:01<00:00,  1.58it/s]\n","                   all        145        235      0.581      0.557      0.466      0.184\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      41/49      2.37G    0.02637      0.032          0         72        320: 100% 28/28 [00:19<00:00,  1.45it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.38it/s]\n","                   all        145        235      0.586      0.545      0.474      0.192\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      42/49      2.37G    0.02637    0.03131          0         96        320: 100% 28/28 [00:19<00:00,  1.44it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.40it/s]\n","                   all        145        235      0.536      0.565      0.459      0.181\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      43/49      2.37G    0.02604    0.03136          0         72        320: 100% 28/28 [00:16<00:00,  1.69it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.36it/s]\n","                   all        145        235      0.561      0.578      0.472       0.18\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      44/49      2.37G    0.02654     0.0318          0         75        320: 100% 28/28 [00:20<00:00,  1.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.29it/s]\n","                   all        145        235      0.575      0.587      0.487      0.189\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      45/49      2.37G    0.02579    0.03104          0         73        320: 100% 28/28 [00:21<00:00,  1.33it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.32it/s]\n","                   all        145        235      0.549      0.587      0.477      0.187\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      46/49      2.37G    0.02565    0.03083          0         97        320: 100% 28/28 [00:16<00:00,  1.66it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.44it/s]\n","                   all        145        235      0.557      0.572      0.492      0.185\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      47/49      2.37G    0.02563    0.03153          0        104        320: 100% 28/28 [00:18<00:00,  1.50it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.24it/s]\n","                   all        145        235      0.614      0.541       0.49      0.194\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      48/49      2.37G    0.02499    0.03077          0         81        320: 100% 28/28 [00:21<00:00,  1.29it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:03<00:00,  1.06s/it]\n","                   all        145        235      0.578      0.562      0.487      0.195\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      49/49      2.37G     0.0255    0.03057          0         87        320: 100% 28/28 [00:16<00:00,  1.72it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.24it/s]\n","                   all        145        235      0.581      0.574      0.489      0.195\n","\n","50 epochs completed in 0.352 hours.\n","Optimizer stripped from runs/train/exp32/weights/last.pt, 14.2MB\n","Optimizer stripped from runs/train/exp32/weights/best.pt, 14.2MB\n","\n","Validating runs/train/exp32/weights/best.pt...\n","Fusing layers... \n","Model summary: 157 layers, 7007428 parameters, 0 gradients\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 3/3 [00:02<00:00,  1.27it/s]\n","                   all        145        235      0.616      0.587      0.542      0.206\n","Results saved to \u001b[1mruns/train/exp32\u001b[0m\n"]}],"source":["!cd yolov5 && python train.py --img 320 --batch 32 --epochs 50 --data carScr_up.yaml --weights yolov5s.pt --hyp '/content/drive/MyDrive/Zummit/Zummit/yolov5/runs/evolve/exp7/hyp_evolve.yaml'"]},{"cell_type":"code","execution_count":26,"id":"EmAl4WYjAVcy","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1243,"status":"ok","timestamp":1672944087827,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"EmAl4WYjAVcy","outputId":"d3fd158e-187e-445f-aa5e-089f1c90d734"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n","YOLOv5 🚀 2023-1-5 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n","\n","Fusing layers... \n","Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n","Adding AutoShape... \n"]}],"source":["model = torch.hub.load('ultralytics/yolov5', 'custom', path = 'yolov5/runs/train/exp27/weights/best.pt')"]},{"cell_type":"code","execution_count":27,"id":"mUR2jgYBAf4c","metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1672944088536,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"mUR2jgYBAf4c"},"outputs":[],"source":["#model"]},{"cell_type":"code","execution_count":28,"id":"A1-Rq1D1Ar4N","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1672944088541,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"A1-Rq1D1Ar4N"},"outputs":[],"source":["img = 'https://t3.ftcdn.net/jpg/02/45/94/96/360_F_245949687_WRQXIFri8xtX3m4oaGgV9Ejun7oIRr1o.jpg'\n","img1 = '/content/drive/MyDrive/Zummit/Zummit/yolov5/data/images/test1.jpg'\n","img2 = '/content/drive/MyDrive/Zummit/Zummit/yolov5/data/images/test2.jpg'"]},{"cell_type":"code","execution_count":35,"id":"Wh-TccrMA_OO","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1672944108574,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"Wh-TccrMA_OO"},"outputs":[],"source":["result = model(img)"]},{"cell_type":"code","execution_count":36,"id":"7rfa_17UBEWv","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1672944109074,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"7rfa_17UBEWv","outputId":"14a66d5a-57f1-4687-bf16-3f6ec2239789"},"outputs":[{"name":"stdout","output_type":"stream","text":["image 1/1: 360x540 2 scratchs\n","Speed: 69.1ms pre-process, 10.5ms inference, 1.3ms NMS per image at shape (1, 3, 448, 640)\n"]}],"source":["print(result)"]},{"cell_type":"code","execution_count":37,"id":"JRr9aXJIBGEC","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"executionInfo":{"elapsed":1669,"status":"ok","timestamp":1672944110727,"user":{"displayName":"Abhinav Yadav","userId":"15669040090700208451"},"user_tz":-330},"id":"JRr9aXJIBGEC","outputId":"c0b2a4cc-28f5-40b7-cf46-19c61df19098"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["%matplotlib inline\n","plt.imshow(np.squeeze(result.render()))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"UbOVEcKjIQ80","metadata":{"id":"UbOVEcKjIQ80"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"03573f4b64923689d4251380d9281a39682538eb2277d996e0c4a1b13dbbb109"}}},"nbformat":4,"nbformat_minor":5}
